{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "Jp0pcBxsVAMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WELCOME\n",
        "\n",
        "This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text.\n",
        "\n",
        "Through two distinct projects, you will explore these technologies and enhance your skills. Detailed descriptions of the projects are provided below."
      ],
      "metadata": {
        "id": "DxOaSxtJWV1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 1: Building a Chatbot with a PDF Document (RAG)\n",
        "\n",
        "In this project, you will develop a chatbot using a provided PDF document from web page. You will utilize the Langchain framework along with a large language model (LLM) such as GPT or Gemini. The chatbot will leverage the Retrieval Augmented Generation (RAG) technique to comprehend the document's content and respond to user queries effectively.\n",
        "\n",
        "### **Project Steps:**\n",
        "\n",
        "- **1.PDF Document Upload:** Upload the provided PDF document from web page (https://aclanthology.org/N19-1423.pdf) (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding).\n",
        "\n",
        "- **2.Chunking:** Divide the uploaded PDF document into smaller segments (chunks). This facilitates more efficient information processing by the LLM.\n",
        "\n",
        "- **3.ChromaDB Setup:**\n",
        "  - Save ChromaDB to your Google Drive.\n",
        "\n",
        "  - Retrieve ChromaDB from your Drive to begin using it in your project.\n",
        "\n",
        "  - ChromaDB serves as a vector database to store embedding vectors generated from your document.\n",
        "\n",
        "- **4.Embedding Vectors Creation:**\n",
        "  - Convert the chunked document into embedding vectors. You can use either GPT or Gemini embedding models for this purpose.\n",
        "\n",
        "  - If you choose the Gemini embedding model, set \"task_type\" to \"retrieval_document\" when converting the chunked document.\n",
        "\n",
        "- **5.Chatbot Development:**\n",
        "  - Utilize the **load_qa_chain** function from the Langchain library to build the chatbot.\n",
        "\n",
        "  - This function will interpret user queries, retrieve relevant information from **ChromaDB**, and generate responses accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "MaCz7nhxKI9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "_eoQWi-uN0dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-community"
      ],
      "metadata": {
        "id": "PCbI4MuNanVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4594bff7-3016-4a65-a377-33ac2e017d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community"
      ],
      "metadata": {
        "id": "qOaahY-AancA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai\n"
      ],
      "metadata": {
        "id": "ZZLMILzfEga_",
        "outputId": "a162082f-7b0a-4fc3-accc-22e1ec10d8be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-1.0.10-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.7.2)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.33 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.2.37)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (0.1.108)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (8.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.20.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.2.2)\n",
            "Downloading langchain_google_genai-1.0.10-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: langchain-google-genai\n",
            "Successfully installed langchain-google-genai-1.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFYZShShqYkz",
        "outputId": "cb936b1d-93e6-4418-dc26-bf92646e7d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pypdfium2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wmw_2hbqYnu",
        "outputId": "ac9230ae-95da-4df9-f448-7ff54a53d610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "zRZkTVi1Our9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "PNsz4Jo4vwhU",
        "outputId": "7be1956a-3985-45e8-bd64-b86e347b32f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.112.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.4)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.64.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.8.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "metadata": {
        "id": "luXDfzEOi83Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "SqA0v6nzRFHf",
        "outputId": "a9298978-c56a-4ff4-9436-ce2c2bef5c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/365.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/318.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kütüphanelerin Yüklenmesi:\n",
        "\n",
        "!pip install -qU langchain-google-community: langchain kütüphanesinin Google ile ilgili topluluk sürümünü yüklüyor. -qU parametresi, sessiz (quiet) ve en güncel sürümü (upgrade) yüklemeyi sağlar.\n",
        "!pip install -qU langchain-community: langchain kütüphanesinin genel topluluk sürümünü yüklüyor.\n",
        "!pip install langchain-google-genai: Google'ın Generative AI (yapay zeka) ile ilgili modüllerini içeren langchain sürümünü yüklüyor.\n",
        "!pip install -qU langchain-chroma: langchain'in Chroma entegrasyonu için gerekli modülleri yüklüyor. Chroma, vektör depolama ve arama için kullanılan bir teknolojidir.\n",
        "!pip install -qU pypdfium2: PDF dosyalarını işlemek için kullanılan pypdfium2 kütüphanesini yüklüyor.\n",
        "!pip install -q -U google-generativeai: Google'ın generative AI API'lerini kullanmak için gerekli olan Python kütüphanesini yüklüyor.\n",
        "!pip install chromadb: Chroma veritabanı için gerekli kütüphaneyi yüklüyor.\n",
        "!pip install -qU langchain-openai: langchain'in OpenAI entegrasyonu için gerekli kütüphaneyi yüklüyor."
      ],
      "metadata": {
        "id": "8BAo6nsto7J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas Kütüphanesinin İçe Aktarılması:\n",
        "\n",
        "import pandas as pd: Verileri işlemek ve analiz etmek için kullanılan popüler bir Python kütüphanesi olan pandas'ı içe aktarıyor."
      ],
      "metadata": {
        "id": "f4T02IN7pBqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain Kütüphanelerinin İçe Aktarılması ve Yapılandırılması:\n",
        "\n",
        "from langchain import PromptTemplate: Yapay zeka modelleriyle çalışırken kullanılacak istem şablonlarını oluşturmak için PromptTemplate sınıfını içe aktarıyor.\n",
        "from langchain.chains.question_answering import load_qa_chain: Soru-cevap zincirini (QA chain) yüklemek için gerekli olan fonksiyonu içe aktarıyor.\n",
        "from langchain.document_loaders import PyPDFLoader: PDF dosyalarını yüklemek ve belgeye dönüştürmek için kullanılan sınıfı içe aktarıyor.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter: Metinleri belirli kurallara göre parçalara ayırmak için kullanılan bir sınıfı içe aktarıyor.\n",
        "from langchain.vectorstores import Chroma: Chroma vektör depolama için kullanılan sınıfı içe aktarıyor.\n",
        "from langchain.chains import RetrievalQA: Geri alma tabanlı soru-cevap sistemi için kullanılan RetrievalQA sınıfını içe aktarıyor.\n",
        "Bu adımların tümü, büyük dil modellerini ve belge işleme sistemlerini kullanarak soru-cevap zincirleri kurmak, belge yüklemek ve metinleri işlemek gibi görevleri yerine getirmek için bir altyapı hazırlamaya yöneliktir. Bu yapılandırma, özellikle Chroma gibi vektör tabanlı arama motorlarını kullanarak belgelerdeki bilgileri etkin bir şekilde aramayı ve işlemeyi amaçlar.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OBJrHa9zpCUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access Google Drive"
      ],
      "metadata": {
        "id": "FuLllnCl2yfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vBdmv-Mu4k_",
        "outputId": "ffb27791-34f1-4c46-ddda-0fa2938cfd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab içinde Google Drive ile ilgili işlemleri yapmanıza olanak tanıyan drive modülünü içe aktarır."
      ],
      "metadata": {
        "id": "WxCKex0Hph9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive'ınızı Colab çalışma ortamınıza bağlar (mount eder). Bu, Google Drive'daki dosyalarınıza Colab üzerinden erişmenizi sağlar. Bağlantı /content/drive dizininde yapılır, böylece Drive'daki dosyalara bu dizin üzerinden erişilebilir."
      ],
      "metadata": {
        "id": "XCZRA6G3pl0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entering Your OpenAI or Google Gemini API Key."
      ],
      "metadata": {
        "id": "0uR9bJp_0MyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('chatbot')"
      ],
      "metadata": {
        "id": "2jwo1SQ2asnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "os.environ['OPENAI_API_KEY']: Python'da çevresel değişkenleri ayarlamak için kullanılır. Bu değişken, genellikle sistem genelinde kullanılmak üzere belirli bir anahtarı veya yapılandırma bilgisini saklar.\n",
        "userdata.get('chatbot'): userdata modülünden chatbot anahtarıyla ilişkili değeri alır. Bu genellikle bir API anahtarı veya başka bir önemli bilgi olur. Bu değeri OPENAI_API_KEY adlı çevresel değişkene atar.\n",
        "Bu kodlar, Google Colab ortamında API anahtarlarını güvenli bir şekilde yönetmek, dosya sistemine erişmek ve Google Drive'a bağlanarak dosyalara erişmek gibi işlemleri gerçekleştirmek için kullanılır. Özellikle, os.environ['OPENAI_API_KEY'] ifadesi, OpenAI API'si gibi harici servislerle entegrasyon yaparken kullanılan anahtarı ayarlamak için kullanılır."
      ],
      "metadata": {
        "id": "937o7KBCpsA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Document"
      ],
      "metadata": {
        "id": "OV9rG-0PN8p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pdf reader function\n",
        "from langchain_community.document_loaders import PyPDFium2Loader\n",
        "\n",
        "def read_doc(directory):\n",
        "    file_loader=PyPDFium2Loader(directory)\n",
        "    pdf_documents=file_loader.load() # PyPDFium2Loader reads page by page\n",
        "    return pdf_documents"
      ],
      "metadata": {
        "id": "5H6eQyYyauxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyPDFium2Loader, PDF dosyalarını sayfa sayfa okuyan bir belge yükleyicisidir. langchain_community.document_loaders modülünden içe aktarılır. Bu, PDF belgelerini analiz etmek ve metin verilerini çıkarmak için kullanılır."
      ],
      "metadata": {
        "id": "Qxiggk3Xp_cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "read_doc(directory): Bir PDF dosyasını okuyup işleyen bir işlev tanımlar.\n",
        "file_loader = PyPDFium2Loader(directory): PyPDFium2Loader sınıfını kullanarak, directory (yani dosya yolu) ile belirtilen PDF dosyasını yükler. Bu, PDF dosyasını okumaya hazırlayan bir nesne oluşturur.\n",
        "pdf_documents = file_loader.load(): load() yöntemi, PDF dosyasını sayfa sayfa okuyarak metin verilerini çıkarır ve bu verileri bir liste olarak döndürür.\n",
        "return pdf_documents: Yüklenen PDF içeriğini döndürür. Bu içerik, her biri bir PDF sayfasını temsil eden öğelerden oluşan bir listedir."
      ],
      "metadata": {
        "id": "5tzvyHNzqEvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf')\n",
        "len(pdf)\n",
        "\n",
        "# The document consists of 16 pages"
      ],
      "metadata": {
        "id": "n_kXJZ5Taupv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d75c85e6-e324-4722-bee5-4ce956fc0a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pdf = read_doc(...): read_doc işlevini çağırarak belirtilen dizindeki PDF dosyasını okur. Burada, dosya yolu /content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf olarak belirtilmiştir. Sonuç olarak, PDF dosyasının tüm sayfalarını içeren bir liste olan pdf değişkeni elde edilir."
      ],
      "metadata": {
        "id": "KSGYV09WqHLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "len(pdf): pdf listesinin uzunluğunu, yani PDF dosyasındaki sayfa sayısını verir. Örnekte, bu PDF dosyasının 16 sayfadan oluştuğu belirtilmiştir."
      ],
      "metadata": {
        "id": "DUjN1ni0qLBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc0CiWk3zU3c",
        "outputId": "2bbdc725-bc45-466d-ca77-5799fe793f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pdf[0]: PDF dosyasının ilk sayfasının içeriğini görüntüler. Liste indeksleme kullanılarak, pdf listesindeki ilk öğe seçilir ve bu öğe o sayfanın içeriğini temsil eder."
      ],
      "metadata": {
        "id": "uMjxUv-wqNEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özetle:\n",
        "Bu kod, bir PDF dosyasını okuyarak her sayfanın metin içeriğini bir liste içinde toplar. read_doc işlevi, PDF dosyasını işlemek için kullanılır ve ardından dosyanın sayfa sayısını kontrol eder ve ilk sayfanın içeriğini görüntüler."
      ],
      "metadata": {
        "id": "vOFgPsHzqNql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Splitter"
      ],
      "metadata": {
        "id": "WLQ1j_JrOF57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "\n",
        "def chunk_data(docs, chunk_size=1000, chunk_overlap=200):\n",
        "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                 chunk_overlap=chunk_overlap)\n",
        "    pdf=text_splitter.split_documents(docs)\n",
        "    return pdf\n"
      ],
      "metadata": {
        "id": "HHQlclU9awwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod parçası, metin belgelerini belirli boyutlarda parçalara ayırmak için kullanılan bir işlev tanımlar. Bu tür işlemler, özellikle büyük metin verileriyle çalışırken, örneğin doğal dil işleme (NLP) modelleri veya bilgi arama sistemleri için önemlidir. Parçalama işlemi, metni anlamlı ve yönetilebilir alt parçalara bölerken aynı zamanda parçalar arasında belli bir bağlamı korumayı amaçlar. İşte bu kodun adım adım açıklaması:\n",
        "RecursiveCharacterTextSplitter ve CharacterTextSplitter: Bu sınıflar, metni karakter bazında böler ve parçalar. RecursiveCharacterTextSplitter, metni daha akıllıca bölmek için bir algoritma kullanır; metni bölerken doğal dilin yapısını dikkate alır.\n"
      ],
      "metadata": {
        "id": "wQIG0rSVqapb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunk_data Fonksiyonu: Bu fonksiyon, bir veya birden fazla belgeyi (metin dokümanları) alır ve bu belgeleri belirli boyutlarda parçalara böler.\n",
        "\n",
        "Parametreler:\n",
        "docs: İşlenecek olan metin belgeleri. Bu, tek bir belge olabileceği gibi, birden fazla belgeyi içeren bir liste de olabilir.\n",
        "chunk_size=1000: Her bir parçanın maksimum karakter sayısını belirler. Örneğin, 1000 karakter olarak ayarlanmış.\n",
        "chunk_overlap=200: Ardışık parçalar arasındaki karakter sayısıdır. Bu, önceki parçanın son 200 karakterinin, sonraki parçanın başında tekrarlanacağı anlamına gelir. Bu tekrar, metnin bölünmesi sırasında bağlam kaybını önlemek için kullanılır.\n",
        "Fonksiyonun İşleyişi:\n",
        "RecursiveCharacterTextSplitter sınıfı, belirli boyutlarda (burada chunk_size=1000) ve belirli bir örtüşme (burada chunk_overlap=200) ile metni böler.\n",
        "split_documents(docs): Bu yöntem, verilen belgeleri böler ve sonuç olarak her biri belirli bir karakter sayısına sahip olan parçalar elde edilir.\n",
        "return pdf: Bölünmüş parçaları döndürür. Bu parçalar, daha sonra başka işlemler veya analizler için kullanılabilir."
      ],
      "metadata": {
        "id": "wGuFvKORqpe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parçalama İşlemi ve chunk_overlap Parametresinin Önemi\n",
        "Parçalama İşlemi: Metin, belirli büyüklüklerdeki parçalara bölünürken, her parçanın anlamlı bir bütünlük içinde olmasına dikkat edilir. RecursiveCharacterTextSplitter, metni doğal dilin yapısını koruyarak bölmek için daha gelişmiş bir yöntem kullanır.\n",
        "\n",
        "chunk_overlap Parametresi: Bu parametre, ardışık parçalar arasında belirli bir örtüşme sağlar. Örneğin, bir parça 1000 karakterlikse ve chunk_overlap=200 olarak belirlenmişse, bu parçanın son 200 karakteri, bir sonraki parçanın başında tekrar yer alır. Bu, metinlerin bölünmesi sırasında bağlamın korunmasına yardımcı olur. Örtüşme, özellikle metnin anlamını korumak ve metni işlerken önemli bilgilerin kaybolmasını önlemek için önemlidir."
      ],
      "metadata": {
        "id": "lZl0u7SpqqVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özetle:\n",
        "Bu kod, büyük metin belgelerini yönetilebilir ve anlamlı parçalara bölmek için kullanılır. Bu işlem, özellikle dil modelleri veya metin analizi gibi işlemlerde metinlerin verimli ve doğru bir şekilde işlenmesini sağlamak için kritik önem taşır. Parçalar arasındaki örtüşme (overlap), bağlamı koruyarak metnin anlamını bozmadan daha büyük metinleri küçük parçalara bölmeyi mümkün kılar."
      ],
      "metadata": {
        "id": "vJwWGQI8quII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc=chunk_data(docs=pdf)\n",
        "len(pdf_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MVmbISDzps_",
        "outputId": "4770fb06-e720-4571-fb0e-f9694af08d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc[25:27]"
      ],
      "metadata": {
        "id": "NaQV6XRwawpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd6e956-5f59-431a-8a46-63ce53861e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 4}, page_content='answering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 4}, page_content='included in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunk_data(docs=pdf): Daha önce PDF'den okunan pdf adlı belgeleri alır ve chunk_data fonksiyonu kullanılarak parçalara böler.\n",
        "Sonuç: pdf_doc, belgelerin parçalanmış halini tutan bir liste olur. Her bir öğe, metnin belirli bir kısmını temsil eden bir parçadır."
      ],
      "metadata": {
        "id": "PahwNZZLrD2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "len(pdf_doc): pdf_doc listesinin uzunluğunu, yani parçalanmış metinlerin sayısını verir. Bu, metnin kaç parçaya bölündüğünü gösterir."
      ],
      "metadata": {
        "id": "oODiauWcrFxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pdf_doc[25:27]: pdf_doc listesindeki 25. ve 26. indekslerdeki parçaları seçer ve döndürür. Python dilinde listeler sıfırdan başladığı için, bu kod 26. ve 27. parçalara denk gelir."
      ],
      "metadata": {
        "id": "v27GBdqPrH4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Parçaların İçeriği ve Meta Verileri:\n",
        "Her parça, bir Document nesnesi olarak temsil edilir ve iki önemli bileşene sahiptir:\n",
        "\n",
        "metadata: Her parçanın meta verilerini içerir. Burada, belgenin kaynağı (PDF dosyasının yolu) ve parçanın hangi sayfaya ait olduğu bilgisi bulunur.\n",
        "\n",
        "source: Parçanın alındığı PDF dosyasının yolu.\n",
        "page: Parçanın ait olduğu sayfa numarası.\n",
        "page_content: Parçalanan metnin kendisi. Bu, belirli bir metin parçasını temsil eder ve chunk_size ve chunk_overlap parametrelerine göre bölünmüştür."
      ],
      "metadata": {
        "id": "qmPiwfVHrL-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "İncelenen Parçaların İçeriği:\n",
        "25. Parça (İlk Parça): PDF'in 4. sayfasına ait bir metin parçası. Bu metin parçası, BERT modelinin ince ayar sonuçlarını ve GLUE benchmark'ı hakkındaki bilgileri içeriyor. Ayrıca, modelin eğitim süresi hakkında bilgiler içeriyor.\n",
        "\n",
        "26. Parça (İkinci Parça): Bu da 4. sayfadan gelen bir başka metin parçası. İlk parça ile örtüşen bazı bilgiler içerir ve BERT modelinin GLUE benchmark'ı üzerindeki ince ayar süreci hakkında daha fazla teknik detay sunar.\n",
        "\n",
        "Parçalama İşlemi ve Bağlamın Korunması:\n",
        "Örtüşme: Parçalar arasında chunk_overlap ile belirlenen bir örtüşme var. Bu, örneğin bir parçanın son kısımlarının bir sonraki parçanın başında tekrarlanması anlamına gelir. Bu örtüşme, metni işlerken bağlamın korunmasına yardımcı olur ve metnin anlam bütünlüğünü bozmadan bölünmesini sağlar.\n",
        "Bu kod, büyük bir PDF belgesini yönetilebilir alt parçalara ayırarak, her bir parçayı ayrı ayrı işlemek ve analiz etmek için kullanılır. Parçalama işlemi sırasında, metinler arasındaki bağlamın korunmasına özen gösterilir."
      ],
      "metadata": {
        "id": "Vh_pWMYYrPdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Creating A Embedding Model\n",
        "### 2. Convert the Each Chunk of The Split Document to Embedding Vectors\n",
        "### 3. Storing of The Embedding Vectors to Vectorstore\n",
        "### 4. Save the Vectorstore to Your Drive"
      ],
      "metadata": {
        "id": "4ENim_5MOT9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
        "                                          dimensions = 3072)# dimensions=256, 1024, 3072\n",
        "\n",
        "\n",
        "\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "96nLFF1ja0k_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1956b7-d973-48d4-bf09-9b624b9a658f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<openai.resources.embeddings.Embeddings object at 0x7ec3ddda13f0> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7ec3dd97b940> model='text-embedding-3-large' dimensions=3072 deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, LangChain kütüphanesini kullanarak metinleri vektörlere dönüştürmek için OpenAI Embeddings modelini yapılandırıyor. Embeddings, metinlerin anlamsal temsillerini sayısal vektörler olarak kodlamaya yarar. Bu tür vektörler, çeşitli doğal dil işleme (NLP) görevlerinde, özellikle benzerlik ölçümleri ve metin tabanlı sorgulamalar gibi işlemlerde kullanılır. Kodun işleyişini adım adım açıklayalım:\n",
        "OpenAIEmbeddings: Bu sınıf, OpenAI'nin embedding modellerini kullanarak metinleri vektörlere dönüştürür. Embedding, kelimelerin veya cümlelerin anlamsal ilişkilerini sayısal bir formda temsil eden bir tekniktir."
      ],
      "metadata": {
        "id": "SQSf1GFjroau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAIEmbeddings: OpenAIEmbeddings sınıfı, metinleri embedding'lere (vektörlere) dönüştürmek için kullanılır. Bu sınıf, OpenAI'nin farklı embedding modellerini destekler.\n",
        "\n",
        "model=\"text-embedding-3-large\": Kullanılacak modelin adı. Burada \"text-embedding-3-large\" modeli seçilmiş. Bu model, büyük boyutlu embedding'ler üretir ve metinleri daha ayrıntılı bir şekilde temsil edebilir.\n",
        "\n",
        "dimensions=3072: Üretilen embedding'lerin boyut sayısını belirler. Bu durumda, her metin vektörü 3072 boyutlu olacaktır. Boyut sayısı, embedding'lerin ne kadar ayrıntılı olduğunu belirler; daha yüksek boyut sayısı, daha fazla bilgi tutabilir, ancak daha fazla hesaplama gücü gerektirebilir.\n",
        "\n",
        "Diğer seçenekler: 256, 1024, 3072 gibi farklı boyutlar da kullanılabilir. Bu, modelin kapasitesine ve uygulamanın gereksinimlerine göre seçilir."
      ],
      "metadata": {
        "id": "JujjkxgEr1in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu satır, oluşturulan embeddings nesnesinin bilgisini terminale veya çıktıya yazdırır. Çıktı, kullanılan model ve boyutlar gibi yapılandırma detaylarını içerir.\n",
        "Özetle:\n",
        "Bu kod, OpenAI'nin embedding modelini kullanarak metinleri yüksek boyutlu vektörlere dönüştürmek için bir yapılandırma yapar. text-embedding-3-large modeli ve 3072 boyutlu vektörler, metinlerin anlamını daha ayrıntılı bir şekilde temsil eder ve bu tür embedding'ler, NLP uygulamalarında metinlerin anlamsal benzerliklerini hesaplamak için kullanılabilir."
      ],
      "metadata": {
        "id": "D2kgF6-hr3b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Vectorstore(index) From Your Drive"
      ],
      "metadata": {
        "id": "y2tMqUthPchD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n"
      ],
      "metadata": {
        "id": "5pjKXmO6a3En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chroma: Chroma, LangChain kütüphanesi içerisinde vektör tabanlı veri depolama ve arama işlemleri için kullanılan bir araçtır. Metin verilerini embedding'ler şeklinde depolar ve bu vektörleri kullanarak hızlı bir şekilde arama yapmaya olanak tanır."
      ],
      "metadata": {
        "id": "WNRhl20psSet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#index=Chroma.from_documents(documents=pdf_doc,\n",
        "#                            embedding=embeddings,\n",
        "#                            persist_directory=\"/content/drive/MyDrive/vectorstore1\") # persist_directory, saves in the directory\n",
        "\n",
        "#retriever=index.as_retriever()"
      ],
      "metadata": {
        "id": "FNxZPnBSQQJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Bu kod parçacığı, Chroma kütüphanesi kullanarak bir vektör deposu (vector store) oluşturmayı ve bu vektör deposu üzerinde arama yapmak için bir retriever (geri getirici) oluşturmayı amaçlar. Kod, iki aşamadan oluşur: önce belgeleri embedding'lere (vektörlere) dönüştürerek vektör deposunu oluşturur, sonra bu depo üzerinde arama işlemleri gerçekleştirmek için bir retriever yaratır."
      ],
      "metadata": {
        "id": "TEUZIC85sltY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod parçacığı, bir dizi belgeyi embedding'lere dönüştürerek bir vektör deposu oluşturur ve ardından bu vektör deposu üzerinde metin arama işlemleri yapmak için bir retriever yaratır.\n",
        "\n",
        "Vektör Deposu Oluşturma: Belgeler embedding'lere dönüştürülür ve belirtilen dizinde kalıcı olarak saklanır.\n",
        "Retriever Oluşturma: Vektör deposunu kullanarak bir retriever oluşturulur, böylece belirli bir sorguya karşılık gelen en uygun metin parçaları hızlıca bulunabilir.\n",
        "Bu yapı, büyük metin koleksiyonları üzerinde hızlı ve etkili aramalar yapmaya olanak tanır."
      ],
      "metadata": {
        "id": "lcjam_ZFstp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index=Chroma(persist_directory=\"/content/drive/MyDrive/vectorstore1\",\n",
        "                    embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "8VTutoj7OV1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chroma(persist_directory=\"/content/drive/MyDrive/vectorstore1\", embedding_function=embeddings):\n",
        "\n",
        "persist_directory: Chroma'nın daha önce depolanan vektör dosyalarını sakladığı dizindir. Burada, /content/drive/MyDrive/vectorstore1 dizini belirtilmiş. Bu dizin, önceden oluşturulmuş bir vektör deposunu içerir.\n",
        "embedding_function=embeddings: Embedding işlemi için kullanılan fonksiyondur. Burada, daha önce yapılandırılan OpenAI embedding modeli (embeddings) kullanılıyor. Bu, verilerin embedding formatına dönüştürülmesinde kullanılır.\n",
        "Bu satır, Chroma vektör deposunu belirli bir dizinden yükler ve belirtilen embedding fonksiyonunu kullanarak metinlerle ilgili arama işlemlerini gerçekleştirmeye hazır hale getirir."
      ],
      "metadata": {
        "id": "I1JjgIM-sVJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_retriver=loaded_index.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "Rl-YMVFnN9au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loaded_index.as_retriever(): Bu metod, Chroma vektör deposunu kullanarak bir retriever (geri getirici) oluşturur. Retriever, belirli bir sorguya en iyi uyan sonuçları döndürmek için kullanılır.\n",
        "\n",
        "search_kwargs={\"k\": 5}: Arama parametrelerini belirtir.\n",
        "\n",
        "k=5: En iyi uyan 5 sonuç geri getirilir. Bu, bir sorgu yapıldığında retriever'ın en uygun 5 metni döndüreceği anlamına gelir."
      ],
      "metadata": {
        "id": "TlRA_7GcsXue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özetle:\n",
        "Bu kod, daha önce depolanan vektör verilerini içeren bir Chroma vektör deposunu yükler ve bu veriler üzerinde metin tabanlı arama işlemleri yapmak için bir retriever oluşturur. Retriever, bir sorguya en iyi uyan vektörleri bulur ve belirli sayıda (bu durumda 5) sonucu döndürür. Bu yapı, büyük metin koleksiyonlarında hızlı ve etkili aramalar yapmaya olanak tanır."
      ],
      "metadata": {
        "id": "4b6p6xiLsYYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrival the First 5 Chunks That Are Most Similar to The User Query from The Document"
      ],
      "metadata": {
        "id": "pKA0PgNJQOmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_retriver = loaded_index.as_retriever(search_kwargs = {'k': 5})"
      ],
      "metadata": {
        "id": "KRH-FWEua5Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loaded_index.as_retriever(): Bu metod, daha önce loaded_index olarak yüklenen vektör deposu üzerinde bir retriever (geri getirici) oluşturur. Bu retriever, sorgulara en uygun belgeleri veya metin parçalarını bulmak için kullanılır.\n",
        "\n",
        "search_kwargs = {'k': 5}: Bu parametre, arama işlemi sırasında kaç sonuç döndürülmesi gerektiğini belirtir. Burada, k=5 olarak ayarlanmış, yani retriever en uygun 5 metin parçasını geri getirecek."
      ],
      "metadata": {
        "id": "vMY0u5tos-b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = load_retriver.invoke('What is BERT?')"
      ],
      "metadata": {
        "id": "80TWdFk6a4-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_retriver.invoke('What is BERT?'): Bu satır, retriever'a 'What is BERT?' (BERT nedir?) şeklinde bir sorgu gönderir. Retriever, bu sorguya en uygun olan 5 metin parçasını (veya belgeyi) vektör deposundan bulur ve geri döndürür.\n",
        "\n",
        "retrieved_docs: Bu, retriever tarafından döndürülen sonuçları tutan bir değişkendir. Bu değişkende, sorguya en uygun metin parçaları (örneğin, BERT ile ilgili açıklamalar) yer alır."
      ],
      "metadata": {
        "id": "8kxVDVx3tA3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "id": "25SNQ8uTR0KB",
        "outputId": "492be121-73d2-476b-82b8-e09f17c7e4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu satır, retriever tarafından döndürülen sonuçları (belgeleri veya metin parçalarını) görüntülemek için kullanılır. Bu belgeler, sorguya karşılık gelen en uygun 5 sonuç olacak ve BERT hakkında bilgiler içerecektir."
      ],
      "metadata": {
        "id": "pZgf2KlmtCzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, daha önce yüklenmiş bir vektör deposundan belirli bir sorguya (bu durumda \"What is BERT?\") en uygun metin parçalarını bulmak için bir retriever kullanır. Retriever, vektör deposunda depolanan metinleri kullanarak, sorguyla en iyi eşleşen 5 metin parçasını döndürür.\n",
        "\n",
        "Retriever Oluşturma: Vektör deposu üzerinde arama işlemi yapacak bir retriever oluşturulur.\n",
        "Sorgu Gönderme: \"What is BERT?\" şeklinde bir sorgu gönderilir.\n",
        "Sonuçların Alınması: Sorguya karşılık gelen en uygun 5 metin parçası geri döndürülür ve retrieved_docs değişkeninde saklanır.\n",
        "Bu süreç, özellikle büyük metin koleksiyonları üzerinde hızlı ve etkili bir şekilde bilgi arama ve getirme işlemleri için kullanılır."
      ],
      "metadata": {
        "id": "dIgI-oFVtMxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating an Answer Based on The Similar Chunks"
      ],
      "metadata": {
        "id": "-G8R4V7BROkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n",
        "\n",
        "----------------\n",
        "{context}\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables =['question','context'],\n",
        "    template = template\n",
        ")"
      ],
      "metadata": {
        "id": "XNDU0jcma7HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, bir kullanıcının sorusuna cevap vermek için belirli bir bağlamda (context) bir şablon (template) oluşturan bir PromptTemplate tanımlar. LangChain kütüphanesi kullanılarak yazılmış bu kod, doğal dil işleme (NLP) uygulamalarında yaygın olarak kullanılır ve kullanıcı tarafından verilen bir soruya verilen bağlamı kullanarak yapılandırılmış bir cevap oluşturur."
      ],
      "metadata": {
        "id": "x6TDDghUtbt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate: Bu sınıf, kullanıcı girdileriyle birleştirilecek bir metin şablonu tanımlamak için kullanılır. Bu şablon, NLP görevlerinde çeşitli amaçlar için kullanılabilir, örneğin bir dil modeline verilecek girişleri oluşturmak için."
      ],
      "metadata": {
        "id": "4reXGZIStqIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "template: Bu değişken, soruya verilen bağlamı kullanarak bir yanıt oluşturmak için kullanılacak olan şablon metnini içerir.\n",
        "\n",
        "\"\"\"Use the following pieces of context to answer the user's question of {question}.\\n\\n----------------\\n{context}\"\"\": Şablonun kendisi şu şekilde yapılandırılmıştır:\n",
        "\n",
        "{question}: Kullanıcının sorusu bu yer tutucuya yerleştirilir.\n",
        "{context}: Soruya yanıt vermek için kullanılacak bağlam bu yer tutucuya yerleştirilir.\n",
        "Şablon şu şekilde çalışır: \"Use the following pieces of context to answer the user's question of {question}.\" (\"Kullanıcının {question} sorusunu yanıtlamak için aşağıdaki bağlamı kullanın.\") yazılır, ardından bir çizgi ile ayrılmış {context} eklenir. Bu, dil modeline açık ve yapılandırılmış bir giriş sağlar."
      ],
      "metadata": {
        "id": "iBa5y2JgtrlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate: Burada, bir PromptTemplate nesnesi oluşturuluyor.\n",
        "input_variables=['question', 'context']: Bu, şablonun hangi değişkenleri beklediğini tanımlar. Şablon, bir question (soru) ve bir context (bağlam) alır.\n",
        "template=template: Bu, yukarıda tanımlanan şablon metnidir."
      ],
      "metadata": {
        "id": "xQjXLVPDtuKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soru ve Bağlamın Yerleştirilmesi: Kullanıcının sorusu ve ilgili bağlam bu şablona yerleştirilir.\n",
        "Cevap Üretme: Bu şablon, özellikle dil modelleriyle kullanıldığında, ilgili bağlamı kullanarak soruya uygun bir cevap oluşturur."
      ],
      "metadata": {
        "id": "Na2QoL1otwO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, kullanıcının sorduğu bir soruya verilen bağlamı kullanarak cevap oluşturmak için bir PromptTemplate oluşturur. Şablon, soruyu ve bağlamı alır, ardından yapılandırılmış bir formatta bu bilgileri birleştirir.\n",
        "\n",
        "{question}: Kullanıcının sorduğu soru.\n",
        "{context}: Soruyu cevaplamak için kullanılan bağlam.\n",
        "Bu yapı, özellikle büyük dil modelleriyle kullanıldığında, kullanıcı sorularına doğru ve bağlama dayalı yanıtlar üretmek için güçlü bir araçtır."
      ],
      "metadata": {
        "id": "aOAOUpy3tzU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline For RAG (If you want, you can use the gemini-1.5-pro model)"
      ],
      "metadata": {
        "id": "sy4fmzsWLayT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\""
      ],
      "metadata": {
        "id": "WDdnBrV-Suxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu satır, our_query adında bir değişken oluşturur ve \"What is BERT?\" (BERT nedir?) metin değerini atar. Bu, sistemin yanıtlamasını istediğimiz soruyu temsil eder."
      ],
      "metadata": {
        "id": "JpGCRcDcuike"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "               temperature=1,\n",
        "               top_p=1)\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "output= chain.invoke({\"question\":our_query, \"context\":retrieved_docs}) # first four most similar texts are returned\n",
        "output"
      ],
      "metadata": {
        "id": "aUnTIxSoa6_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c7df5639-865a-4384-dd2a-db12394627b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT, which stands for Bidirectional Encoder Representations from Transformers, is a natural language processing (NLP) model developed by Google. It utilizes a transformer architecture and is designed to understand the context of words in a sentence by considering the relationships between all the words in both directions (left-to-right and right-to-left). This bidirectional approach allows BERT to capture nuanced meanings and improve performance on a variety of language tasks, such as question answering and language inference. It has become a foundational model in NLP and has significantly advanced the state of the art in many applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu satır, langchain_openai paketinden ChatOpenAI sınıfını içeri aktarır. ChatOpenAI, muhtemelen OpenAI'nin dil modelleri ile çalışmak için kullanılan özel bir sınıftır.\n",
        "from langchain_core.output_parsers import StrOutputParser:\n",
        "\n",
        "Bu satır, langchain_core.output_parsers paketinden StrOutputParser sınıfını içeri aktarır. Bu sınıf, dil modelinin çıktısını bir metin formatında işlemek ve formatlamak için kullanılır.\n",
        "Bu satır, ChatOpenAI sınıfından bir örnek oluşturur. Bu örnek, kullanmak istediğimiz dil modelini temsil eder ve bazı parametreler ile yapılandırılır:\n",
        "model_name=\"gpt-4o-mini\": Kullanılacak modelin adını belirtir, burada \"gpt-4o-mini\" (GPT-4'ün daha küçük bir versiyonu) kullanılıyor.\n",
        "temperature=1: Modelin çıktısındaki rastgeleliği kontrol eder. Daha yüksek bir değer daha rastgele sonuçlar üretir.\n",
        "top_p=1: Bu, bir örnekleme tekniği parametresidir. 1 olarak ayarlandığında, model tüm olasılık dağılımını dikkate alır."
      ],
      "metadata": {
        "id": "hzPFmAWyulV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=1, top_p=1):\n",
        "\n",
        "Bu satır, ChatOpenAI sınıfından bir örnek oluşturur. Bu örnek, kullanmak istediğimiz dil modelini temsil eder ve bazı parametreler ile yapılandırılır:\n",
        "model_name=\"gpt-4o-mini\": Kullanılacak modelin adını belirtir, burada \"gpt-4o-mini\" (GPT-4'ün daha küçük bir versiyonu) kullanılıyor.\n",
        "temperature=1: Modelin çıktısındaki rastgeleliği kontrol eder. Daha yüksek bir değer daha rastgele sonuçlar üretir.\n",
        "top_p=1: Bu, bir örnekleme tekniği parametresidir. 1 olarak ayarlandığında, model tüm olasılık dağılımını dikkate alır."
      ],
      "metadata": {
        "id": "0XZQlaWiuwsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain = prompt_template | llm | StrOutputParser():\n",
        "\n",
        "Bu satır, bir \"zincir\" oluşturur. Bu bağlamda:\n",
        "prompt_template (kodda tanımlanmamış, ancak muhtemelen istem için bir şablon içeriyor).\n",
        "llm daha önce oluşturduğumuz dil modeli nesnesi.\n",
        "StrOutputParser() ise, çıktıyı bir metin formatında işleyecek olan çıktı ayrıştırıcıdır.\n",
        "| operatörü, burada ardışık bir yapı kurmaya yarıyor; bir bileşenin çıktısı, bir sonraki bileşene giriş olarak aktarılır."
      ],
      "metadata": {
        "id": "xOaDA7qiuyt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "output= chain.invoke({\"question\":our_query, \"context\":retrieved_docs}):\n",
        "\n",
        "Bu satır, zinciri belirtilen girdilerle çalıştırır:\n",
        "question: Daha önce başlattığımız sorgu (\"What is BERT?\").\n",
        "context: retrieved_docs, modelin uygun bir yanıt oluşturmak için kullanacağı bazı belgeler veya bağlamı tutan bir değişken.\n",
        "Bu işlemin çıktısı output değişkenine kaydedilir."
      ],
      "metadata": {
        "id": "9f1JxlG_u0Oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from IPython.display import Markdown:\n",
        "\n",
        "Bu satır, IPython gösterim modülünden Markdown sınıfını içeri aktarır. Bu sınıf, çıktının Jupyter Notebook veya benzeri ortamlarda Markdown formatında gösterilmesine olanak tanır.\n",
        "Markdown(output):\n",
        "\n",
        "Bu satır, dil modelinden gelen output (bir metin çıktısı) değerini Markdown formatına dönüştürerek gösterir."
      ],
      "metadata": {
        "id": "_hbjqjReu2gC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özet: Kod, bir doğal dil işleme (NLP) boru hattı (pipeline) kullanarak bir sorgu gerçekleştirmek için tasarlanmıştır:\n",
        "\n",
        "Bir sorgu ile başlanır (\"What is BERT?\").\n",
        "Bu sorgu, önceden tanımlanmış bir istem şablonu ve OpenAI'nin dil modeli (gpt-4o-mini) kullanılarak işlenir.\n",
        "Dil modeli, belirtilen bağlamı (retrieved_docs) kullanarak uygun bir yanıt üretir.\n",
        "Yanıt, bir metin formatına dönüştürülür ve Markdown olarak gösterilir.\n",
        "Bu zincir tabanlı yaklaşım, özellikle konuşma AI veya Soru-Cevap (Q&A) sistemlerinde yaygındır. Farklı bileşenler (bağlam alma, model çağrısı, çıktı formatlama gibi) sırayla ele alınarak doğru ve kullanıcı dostu sonuçlar sağlanır."
      ],
      "metadata": {
        "id": "mES0fZ3lu4My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(output)"
      ],
      "metadata": {
        "id": "5UrWtZ776nil",
        "outputId": "df632308-e1cb-4806-bffe-3e5fc3e1296a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a natural language processing (NLP) model developed by Google. It utilizes a transformer architecture and is designed to understand the context of words in a sentence by considering the relationships between all the words in both directions (left-to-right and right-to-left). This bidirectional approach allows BERT to capture nuanced meanings and improve performance on a variety of language tasks, such as question answering and language inference. It has become a foundational model in NLP and has significantly advanced the state of the art in many applications."
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kodun son satırında yapılan işlem şu şekilde açıklanabilir:\n",
        "\n",
        "Markdown(output) Satırının Açıklaması\n",
        "from IPython.display import Markdown:\n",
        "Bu satır, Jupyter Notebook veya benzeri bir IPython ortamında Markdown formatındaki metinleri görüntülemek için kullanılan Markdown sınıfını içeri aktarır.\n",
        "Markdown(output):\n",
        "output değişkeni, önceki adımlarda dil modelinden (örneğin, BERT gibi bir modelin ne olduğu hakkında bilgi veren bir model) alınan yanıtı içerir. Bu yanıt, muhtemelen bir metin olarak formatlanmıştır.\n",
        "Markdown(output), bu metni Markdown formatında görüntüler. Markdown, metinleri biçimlendirmek için kullanılan bir işaretleme dilidir ve başlıklar, kalın veya italik metinler, listeler ve bağlantılar gibi farklı stiller eklemek için kullanılır."
      ],
      "metadata": {
        "id": "LYBRpw76vWTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markdown(output) komutu, modelin yanıtını daha kullanıcı dostu bir biçimde gösterir ve bu sayede yanıtın içeriği anlaşılır ve görsel olarak düzenli hale gelir."
      ],
      "metadata": {
        "id": "TSINbIw-voWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT, \"Bidirectional Encoder Representations from Transformers\" ifadesinin kısaltmasıdır ve Google tarafından geliştirilen bir doğal dil işleme (NLP) modelidir. BERT, kelimelerin bir cümledeki bağlamını anlamak için bir \"transformer\" mimarisi kullanır. Bu mimari, kelimelerin hem sağdan sola hem de soldan sağa olan ilişkilerini dikkate alarak, cümlenin tamamındaki tüm kelimeler arasındaki bağlantıları öğrenir."
      ],
      "metadata": {
        "id": "qcCt9cPpvWXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Retrieval**"
      ],
      "metadata": {
        "id": "8vbuPptARJrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_query(query,k=5):\n",
        "    load_retriever=loaded_index.as_retriever(search_kwargs={\"k\": k}) #loaded_index\n",
        "    return load_retriever.invoke(query)"
      ],
      "metadata": {
        "id": "VYc2ddhORJGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amaç: Kullanıcının sorgusuna en yakın k sayıda belgeyi geri döndürmek.\n",
        "Parametreler:\n",
        "query: Kullanıcının sorduğu soru veya sorgu (örneğin: \"What is BERT?\").\n",
        "k: Kaç tane benzer belgenin döndürüleceğini belirtir (varsayılan olarak 5).\n",
        "İşleyiş:\n",
        "loaded_index.as_retriever(...): loaded_index adı verilen önceden yüklenmiş bir arama indeksini kullanarak bir \"retriever\" (geri getirici) oluşturur. Bu retriever, belirtilen k sayıda en benzer belgeyi bulmaya yarar.\n",
        "invoke(query): Belirtilen sorguya göre en benzer belgeleri bulur ve geri döndürür."
      ],
      "metadata": {
        "id": "zb46COAhv3ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=5) # first two most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEyWH1V7Rgv_",
        "outputId": "35ff071a-0918-4f84-9324-9e9b0a5aab66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amaç: \"What is BERT?\" sorusuna en uygun olan 5 belgeyi bulmak.\n",
        "İşleyiş:\n",
        "retrieve_query fonksiyonu kullanılarak our_query sorgusu çalıştırılır ve en benzer 5 belge doc_search değişkenine atanır."
      ],
      "metadata": {
        "id": "5wWGxTvsv7FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answers(query, k = 5):\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from IPython.display import Markdown\n",
        "\n",
        "    doc_search=retrieve_query(query, k = k) # most similar texts are returned\n",
        "\n",
        "\n",
        "    template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n",
        "\n",
        "    ----------------\n",
        "    {context}\"\"\"\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "    input_variables =['question','context'],\n",
        "    template = template)\n",
        "\n",
        "\n",
        "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "                  temperature=0,\n",
        "                  top_p=1)\n",
        "\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    output= chain.invoke({\"question\":query, \"context\":doc_search}) # first four most similar texts are returned\n",
        "    return Markdown(output)"
      ],
      "metadata": {
        "id": "Na1X4-QXOo-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amaç: Kullanıcının sorusuna (örneğin, \"BERT'in temel mimari bileşenleri nelerdir?\") en uygun yanıtı oluşturmak.\n",
        "Parametreler:\n",
        "query: Kullanıcının sorduğu soru.\n",
        "k: En benzer k sayıda belgeyi geri getirmek için kullanılır."
      ],
      "metadata": {
        "id": "q0iG6m8gv99h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonksiyonun İçeriği:\n",
        "Gerekli Kütüphanelerin İçe Aktarılması:\n",
        "\n",
        "langchain_openai, langchain_core.output_parsers, langchain.prompts ve IPython.display modüllerini içe aktararak, OpenAI modelini kullanmak, çıktıyı işlemek ve Markdown formatında görüntülemek için gerekli kütüphaneler yüklenir.\n",
        "Benzer Belgeleri Getirme:\n",
        "\n",
        "doc_search = retrieve_query(query, k=k) ifadesi ile kullanıcı sorusuna en uygun belgeler (k sayıda) doc_search değişkenine atanır.\n",
        "Şablon Tanımlama ve Kullanımı:\n",
        "\n",
        "template değişkeni, dil modelinin kullanıcı sorusuna yanıt vermesi için kullanacağı bir metin şablonunu tanımlar. Şablon, kullanıcının sorusunu ({question}) ve belgeleri ({context}) içerir.\n",
        "PromptTemplate: Bu şablon kullanılarak bir istem şablonu (prompt_template) oluşturulur.\n",
        "Dil Modelinin Yapılandırılması:\n",
        "\n",
        "ChatOpenAI: GPT-4'ün bir versiyonunu (gpt-4o-mini) çalıştırmak için kullanılır. temperature=0 ayarı, modelin daha belirgin ve deterministik (yani rastgeleliği düşük) yanıtlar üretmesini sağlar.\n",
        "top_p=1: Modelin çıktılarını üretirken tüm olasılık dağılımını dikkate almasını sağlar.\n",
        "İşlem Zincirinin Oluşturulması ve Çalıştırılması:\n",
        "\n",
        "chain: Sorgu yanıtlamada kullanılacak bir zincir (pipeline) oluşturur. Bu zincir, istem şablonu (prompt_template), dil modeli (llm), ve çıktı ayrıştırıcısını (StrOutputParser()) içerir.\n",
        "chain.invoke(...): Kullanıcının sorusu ve bağlam olarak bulunan belgelerle (doc_search) zinciri çalıştırır ve modelden bir yanıt döndürür.\n",
        "Markdown Formatında Yanıt Döndürme:\n",
        "\n",
        "Markdown(output): Modelin çıktısını Markdown formatında döndürerek Jupyter Notebook veya benzeri bir ortamda düzgün bir şekilde görüntülenmesini sağlar."
      ],
      "metadata": {
        "id": "GFLLaUBdv-dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What are the key architectural components of BERT?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "KUqWSAW-QGin",
        "outputId": "44be8eef-7ee1-4fd5-cac2-cdecf565520f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT (Bidirectional Encoder Representations from Transformers) is built on several key architectural components:\n\n1. **Transformer Architecture**: BERT is based on the Transformer model, which uses self-attention mechanisms to process input data. This allows BERT to consider the context of a word based on all other words in a sentence, rather than just the words that come before or after it.\n\n2. **Bidirectional Context**: Unlike previous models that read text in a unidirectional manner (left-to-right or right-to-left), BERT reads text in both directions simultaneously. This bidirectional approach enables a deeper understanding of context and meaning.\n\n3. **Input Representation**: BERT uses a unique input representation that combines three types of embeddings:\n   - **Token Embeddings**: Represent individual words or subwords.\n   - **Segment Embeddings**: Indicate whether a token belongs to the first or second sentence in tasks involving pairs of sentences.\n   - **Position Embeddings**: Provide information about the position of each token in the sequence, as the Transformer architecture does not inherently understand the order of tokens.\n\n4. **Multi-Head Self-Attention**: BERT employs multi-head self-attention, which allows the model to focus on different parts of the input sequence simultaneously. This enhances its ability to capture various relationships and dependencies between words.\n\n5. **Layer Normalization and Residual Connections**: Each layer in BERT includes layer normalization and residual connections, which help stabilize training and allow for deeper networks by mitigating the vanishing gradient problem.\n\n6. **Pre-training and Fine-tuning**: BERT is pre-trained on a large corpus of text using unsupervised learning tasks (Masked Language Model and Next Sentence Prediction) and can be fine-tuned on specific tasks with labeled data, making it highly versatile for various NLP applications.\n\nThese components work together to enable BERT to achieve state-of-the-art performance on a wide range of natural language processing tasks."
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amaç: \"BERT'in temel mimari bileşenleri nelerdir?\" sorusuna yanıt almak.\n",
        "İşleyiş:\n",
        "get_answers fonksiyonu çağrılarak our_query kullanılır ve yanıt answer değişkenine atanır.\n",
        "Yanıt, Markdown formatında görüntülenir, böylece görsel olarak düzenlenmiş ve okunabilir bir yanıt elde edilir."
      ],
      "metadata": {
        "id": "bT6FoC8GwCD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, bir sorgu alır, en benzer belgeleri bulur, dil modelini kullanarak bu belgelerden bir yanıt oluşturur ve sonucu Markdown formatında görüntüler. Kullanıcı, bu sayede açık ve düzenli bir şekilde sorusunun yanıtını alır."
      ],
      "metadata": {
        "id": "GG8u5v_RwEIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What are the main contributions of the BERT model?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "gbTmfKHPRvBO",
        "outputId": "d5aa0146-3f8e-4bdd-e729-e4e71e81c7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The BERT (Bidirectional Encoder Representations from Transformers) model has made several significant contributions to the field of natural language processing (NLP):\n\n1. **Bidirectional Contextual Understanding**: Unlike previous models that processed text in a unidirectional manner (left-to-right or right-to-left), BERT reads text in both directions simultaneously. This allows it to capture the context of a word based on all surrounding words, leading to a deeper understanding of language.\n\n2. **Pre-training and Fine-tuning Paradigm**: BERT introduced a two-step approach where the model is first pre-trained on a large corpus of text using unsupervised learning techniques (like masked language modeling and next sentence prediction) and then fine-tuned on specific tasks with labeled data. This approach has proven effective for a variety of NLP tasks.\n\n3. **Transfer Learning in NLP**: BERT popularized the use of transfer learning in NLP, allowing models to leverage knowledge gained from pre-training on large datasets to improve performance on specific tasks with limited data.\n\n4. **State-of-the-Art Performance**: BERT achieved state-of-the-art results on several benchmark NLP tasks, including question answering, sentiment analysis, and named entity recognition, demonstrating its effectiveness and versatility.\n\n5. **Open Source and Community Impact**: BERT was released as an open-source model, which has encouraged widespread adoption and further research in the NLP community. It has inspired numerous variations and improvements, leading to advancements in the field.\n\nOverall, BERT has significantly advanced the capabilities of NLP models and has influenced the development of subsequent architectures and techniques."
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, \"BERT modelinin başlıca katkıları nelerdir?\" sorusuna en uygun yanıtı bulmak için bir doğal dil işleme boru hattı kullanır. Bu, kullanıcıdan gelen sorguya benzer belgeleri bulur, dil modelini kullanarak bu belgelere dayalı bir yanıt oluşturur ve yanıtı Markdown formatında gösterir. Bu sayede, kullanıcı dostu ve anlaşılır bir sonuç elde edilir."
      ],
      "metadata": {
        "id": "O4mU_I-AwRao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our_query adlı bir değişken tanımlanır ve \"What are the main contributions of the BERT model?\" değeri atanır. Bu değişken, dil modelinin yanıtlamasını istediğimiz soruyu içerir."
      ],
      "metadata": {
        "id": "MIoDcmYrwYo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_answers fonksiyonu çağrılır ve içine our_query değişkeni geçirilir.\n",
        "get_answers fonksiyonu daha önce tanımlanmıştı ve şu adımları gerçekleştiriyordu:\n",
        "En Benzer Belgeleri Getirme: Soruya en uygun belgeleri (k sayıda, varsayılan olarak 5) bulmak için retrieve_query fonksiyonunu kullanır.\n",
        "Şablon Kullanarak Yanıt Oluşturma: Kullanıcının sorusu ve bulunan belgeler kullanılarak, dil modeli (gpt-4o-mini) bir yanıt oluşturur.\n",
        "Markdown Formatında Yanıt Döndürme: Yanıt, Markdown formatında düzenlenir ve okunabilir bir şekilde döndürülür."
      ],
      "metadata": {
        "id": "q-kUPxUnwa_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer değişkeni, get_answers fonksiyonunun çıktısını tutar ve bu değişken görüntülenerek, dil modelinin verdiği yanıt gösterilir."
      ],
      "metadata": {
        "id": "-QyfXdtOwdIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_answers Fonksiyonunun Çalışma Mantığı\n",
        "get_answers fonksiyonu çağrıldığında şu adımları gerçekleştirir:\n",
        "\n",
        "Soruya En Uygun Belgeleri Bulur:\n",
        "\n",
        "retrieve_query fonksiyonu, \"What are the main contributions of the BERT model?\" sorusuna en benzer belgeleri döndürür.\n",
        "Yanıt Şablonunu Oluşturur ve Kullanır:\n",
        "\n",
        "Kullanıcının sorusunu ve geri döndürülen belgeleri kullanarak bir yanıt oluşturur.\n",
        "Şablon:\n",
        "python\n",
        "Kodu kopyala\n",
        "template = \"\"\"Use the following pieces of context to answer the user's question of {question}.\n",
        "----------------\n",
        "{context}\"\"\"\n",
        "Bu şablon, dil modelinin yanıt verirken kullanması için bir yapı sağlar. Model, kullanıcının sorusuna yanıt verirken bu şablonun içindeki question (soru) ve context (bağlam) yer tutucularını kullanır.\n",
        "Dil Modeli (LLM) Kullanılarak Yanıt Üretilir:\n",
        "\n",
        "gpt-4o-mini modeli kullanılarak, bağlam ve soru birlikte işlenir ve modelden bir yanıt alınır.\n",
        "Yanıt Markdown Formatında Döndürülür:\n",
        "\n",
        "Çıktı, Markdown formatında düzenlenir ve kullanıcıya daha okunabilir bir formatta sunulur."
      ],
      "metadata": {
        "id": "ytT_eeSjwhby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 2: Generating PDF Document Summaries\n",
        "\n",
        "In this project, you will explore various methods for creating summaries from the provided PDF document. You will experiment with different chaining functions offered by the Langchain library to achieve this.\n",
        "\n",
        "### **Project Steps:**\n",
        "- **1.PDF Document Upload and Chunking:** As in the first project, upload the PDF document and divide it into smaller chunks. Consider splitting it by half-page or page.\n",
        "\n",
        "- **2.Summarization Techniques:**\n",
        "\n",
        "  - **Summary of the First 5 Pages (Stuff Chain):** Utilize the load_summarize_chain function with the parameter chain_type=\"stuff\" to generate a concise summary of the first 5 pages of the PDF document.\n",
        "\n",
        "  - **Short Summary of the Entire Document (Map Reduce Chain):** Employ chain_type=\"map_reduce\" and refine parameters to create a brief summary of the entire document. This method generates individual summaries for each chunk and then combines them into a final summary.\n",
        "\n",
        "  - **Detailed Summary with Bullet Points (Map Reduce Chain):** Use chain_type=\"map_reduce\" to generate a detailed summary with at least 1000 tokens. Provide the LLM with the prompt \"Summarize with 1000 tokens\" and set the max_token parameter to a value greater than 1000. Add a title to the summary and present key points using bullet points.\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "- Models like GPT-4 and Gemini Pro models might excel in generating summaries based on token count. Consider prioritizing these models.\n",
        "\n",
        "- For comprehensive information on Langchain and LLMs, refer to their respective documentation.\n",
        "Best of luck!"
      ],
      "metadata": {
        "id": "H9GmKlL2NRff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "WhjLe0IqRnl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ZXdV8CcqbFrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu bileşenler, LangChain framework'ünün bir parçasıdır ve doğal dil işleme (NLP) sistemleri için bilgi getirme ve yanıt oluşturma görevlerini optimize etmek için kullanılır. create_retrieval_chain ve create_stuff_documents_chain belgeleri bulma ve işleme görevlerini gerçekleştirirken, ChatPromptTemplate modeli doğru yönlendirecek istem şablonlarını oluşturur. Bu sayede, kullanıcının sorgusuna en uygun ve anlamlı yanıtın oluşturulması sağlanır."
      ],
      "metadata": {
        "id": "fq2gLLSUw3cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". from langchain.chains import create_retrieval_chain\n",
        "Açıklama:\n",
        "\n",
        "create_retrieval_chain fonksiyonu, LangChain kütüphanesindeki bir retrieval zincirini oluşturmak için kullanılır. Bu zincir, bir kullanıcının sorgusuna en uygun yanıtları oluşturabilmek için önce ilgili belgeleri (dokümanları) geri getirir (retrieval).\n",
        "Amaç:\n",
        "\n",
        "Sorguya uygun belgeleri (dokümanları) bulmak ve bu belgeleri kullanarak daha sonrasında dil modeline geçiş yapmak.\n",
        "Genellikle, bir bilgi tabanında veya büyük bir belge koleksiyonunda uygun içeriği bulmak için kullanılır."
      ],
      "metadata": {
        "id": "LzlZbg2axbDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "Açıklama:\n",
        "\n",
        "create_stuff_documents_chain fonksiyonu, LangChain kütüphanesindeki bir belge birleştirme zinciri oluşturur. Bu zincir, bulunan belgeleri veya dokümanları bir araya getirip, anlamlı bir bütün oluşturur.\n",
        "Amaç:\n",
        "\n",
        "Bulunan belgeleri işleyip, birleştirerek tek bir yanıt için gerekli bağlamı oluşturmak.\n",
        "Bu işlem, belgelerin veya dokümanların farklı kısımlarını bir araya getirip, dil modelinin daha anlamlı ve tutarlı bir yanıt vermesine yardımcı olur."
      ],
      "metadata": {
        "id": "cLNh3MHAxbub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "Açıklama:\n",
        "\n",
        "ChatPromptTemplate, LangChain'in core (çekirdek) kütüphanesindeki bir sınıftır ve sohbet tabanlı istemleri (prompts) oluşturmak için kullanılır.\n",
        "Sohbet tabanlı istemler, dil modeline spesifik bir görev veya soru için verilen yapılandırılmış metinlerdir.\n",
        "Amaç:\n",
        "\n",
        "Kullanıcının sorusunu ve modeli yanıtlaması gereken bağlamı tanımlayan bir istem şablonu oluşturmak.\n",
        "ChatPromptTemplate, kullanıcıdan gelen sorulara veya görevlere uygun istemler hazırlamayı kolaylaştırır."
      ],
      "metadata": {
        "id": "nJVB6vZ0xgge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Document"
      ],
      "metadata": {
        "id": "yqImlx_IRqQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf')"
      ],
      "metadata": {
        "id": "CCkT3msfbH_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e046cc-2aa0-447a-8ff2-e6f4097f3661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama:\n",
        "\n",
        "read_doc fonksiyonu, verilen bir PDF dosyasını okur ve dosyanın içeriğini Python'da işleyebilecek bir formata dönüştürür.\n",
        "'/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf' dosya yolu, Google Drive üzerinde bulunan bir PDF dosyasının konumunu belirtir.\n",
        "pdf değişkeni, bu PDF dosyasının okunmuş içeriğini tutar.\n",
        "Amaç:\n",
        "\n",
        "Bu satır, belirli bir PDF dosyasını okumak ve üzerinde işlem yapmak için kullanılır. Genellikle, dil işleme (NLP) görevlerinde, bir PDF dosyasından metin çıkarıp analiz etmek için kullanılır."
      ],
      "metadata": {
        "id": "o_PMoUfmxuf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[0] #first page"
      ],
      "metadata": {
        "id": "5a_FpBOcbHzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24496c6a-048c-40c8-ed12-ed1819faf2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". pdf[0] #first page\n",
        "Açıklama:\n",
        "\n",
        "pdf[0], PDF belgesinin ilk sayfasını temsil eder. Bu, pdf değişkeninde saklanan içeriğin ilk sayfasını alır.\n",
        "#first page (ilk sayfa) ifadesi bir yorumdur ve kodun ne yaptığını açıklamak için kullanılır.\n",
        "Amaç:\n",
        "\n",
        "PDF dosyasının yalnızca ilk sayfasının içeriğini elde etmek için kullanılır. Örneğin, bir araştırma makalesinin özetini veya başlık sayfasını almak isteyebilirsiniz."
      ],
      "metadata": {
        "id": "VUNno7mjxvh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özet\n",
        "Bu kod parçaları, bir PDF dosyasını okumak ve içeriğiyle çalışmak için kullanılır.\n",
        "\n",
        "Dosyanın Okunması: read_doc fonksiyonu kullanılarak PDF dosyası okunur.\n",
        "İlk Sayfanın Elde Edilmesi: pdf[0] ifadesi, bu PDF dosyasının ilk sayfasının içeriğini alır ve işleme hazır hale getirir."
      ],
      "metadata": {
        "id": "FYOmS33nxxxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing the First 5 Pages of The Document With Chain_Type of The 'stuff'"
      ],
      "metadata": {
        "id": "LuyT0IoWR4n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import load_summarize_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "O3yAnW3PbKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RecursiveCharacterTextSplitter sınıfı, bir metni belirli karakter sınırlarına göre bölmek için kullanılan bir metin ayırıcıdır.\n",
        "Bu sınıf, metni daha küçük parçalara bölmek için kullanılır; özellikle uzun metinlerin veya belgelerin işlenmesi gerektiğinde yararlıdır. Metni, maksimum karakter uzunluğunu aşmayan parçalara bölerek, modelin daha verimli çalışmasını sağlar."
      ],
      "metadata": {
        "id": "a2gG-_q3yWCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özetleme veya başka bir dil işleme görevi için metni daha küçük parçalara ayırmak. Böylece, model her bir parça üzerinde ayrı ayrı çalışabilir ve belgenin tamamı için daha iyi bir özet veya analiz yapabilir."
      ],
      "metadata": {
        "id": "Vq1Le2ZyyXJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_summarize_chain fonksiyonu, LangChain kütüphanesinde özetleme (summarization) zinciri oluşturmak için kullanılır.\n",
        "Bir özetleme zinciri, verilen metni veya belgeyi kısaca özetleyen bir süreçtir. Bu zincir, metnin veya belgenin uzunluğunu ve karmaşıklığını azaltarak daha kısa bir açıklama veya özet sağlar."
      ],
      "metadata": {
        "id": "i7-rMDIVyaXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Büyük metinleri veya belgeleri otomatik olarak özetlemek ve kullanıcıya daha kısa ve anlaşılır bir bilgi sunmak."
      ],
      "metadata": {
        "id": "kfP2IxUIycKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI sınıfı, OpenAI dil modellerine erişmek ve onları kullanmak için LangChain'de sağlanan bir arayüzdür.\n",
        "Bu sınıf, OpenAI'nin modellerine (örneğin, GPT-3, GPT-4) bağlanarak, dil işleme görevleri için kullanılır."
      ],
      "metadata": {
        "id": "gfRs3LOXyd3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI'nin güçlü dil modellerini kullanarak metin işleme görevlerini (örneğin özetleme, soru-cevap) gerçekleştirmek."
      ],
      "metadata": {
        "id": "w-jLOj9Ayf5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[:5] # first five page"
      ],
      "metadata": {
        "id": "8wopgGPibKA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aecaf6c8-3769-4996-a9b6-3156eb94acec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 1}, page_content='4172\\r\\nword based only on its context. Unlike left-to\\x02right language model pre-training, the MLM ob\\x02jective enables the representation to fuse the left\\r\\nand the right context, which allows us to pre\\x02train a deep bidirectional Transformer. In addi\\x02tion to the masked language model, we also use\\r\\na “next sentence prediction” task that jointly pre\\x02trains text-pair representations. The contributions\\r\\nof our paper are as follows:\\r\\n• We demonstrate the importance of bidirectional\\r\\npre-training for language representations. Un\\x02like Radford et al. (2018), which uses unidirec\\x02tional language models for pre-training, BERT\\r\\nuses masked language models to enable pre\\x02trained deep bidirectional representations. This\\r\\nis also in contrast to Peters et al. (2018a), which\\r\\nuses a shallow concatenation of independently\\r\\ntrained left-to-right and right-to-left LMs.\\r\\n• We show that pre-trained representations reduce\\r\\nthe need for many heavily-engineered task\\x02specific architectures. BERT is the first fine\\x02tuning based representation model that achieves\\r\\nstate-of-the-art performance on a large suite\\r\\nof sentence-level and token-level tasks, outper\\x02forming many task-specific architectures.\\r\\n• BERT advances the state of the art for eleven\\r\\nNLP tasks. The code and pre-trained mod\\x02els are available at https://github.com/\\r\\ngoogle-research/bert.\\r\\n2 Related Work\\r\\nThere is a long history of pre-training general lan\\x02guage representations, and we briefly review the\\r\\nmost widely-used approaches in this section.\\r\\n2.1 Unsupervised Feature-based Approaches\\r\\nLearning widely applicable representations of\\r\\nwords has been an active area of research for\\r\\ndecades, including non-neural (Brown et al., 1992;\\r\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\r\\nneural (Mikolov et al., 2013; Pennington et al.,\\r\\n2014) methods. Pre-trained word embeddings\\r\\nare an integral part of modern NLP systems, of\\x02fering significant improvements over embeddings\\r\\nlearned from scratch (Turian et al., 2010). To pre\\x02train word embedding vectors, left-to-right lan\\x02guage modeling objectives have been used (Mnih\\r\\nand Hinton, 2009), as well as objectives to dis\\x02criminate correct from incorrect words in left and\\r\\nright context (Mikolov et al., 2013).\\r\\nThese approaches have been generalized to\\r\\ncoarser granularities, such as sentence embed\\x02dings (Kiros et al., 2015; Logeswaran and Lee,\\r\\n2018) or paragraph embeddings (Le and Mikolov,\\r\\n2014). To train sentence representations, prior\\r\\nwork has used objectives to rank candidate next\\r\\nsentences (Jernite et al., 2017; Logeswaran and\\r\\nLee, 2018), left-to-right generation of next sen\\x02tence words given a representation of the previous\\r\\nsentence (Kiros et al., 2015), or denoising auto\\x02encoder derived objectives (Hill et al., 2016).\\r\\nELMo and its predecessor (Peters et al., 2017,\\r\\n2018a) generalize traditional word embedding re\\x02search along a different dimension. They extract\\r\\ncontext-sensitive features from a left-to-right and a\\r\\nright-to-left language model. The contextual rep\\x02resentation of each token is the concatenation of\\r\\nthe left-to-right and right-to-left representations.\\r\\nWhen integrating contextual word embeddings\\r\\nwith existing task-specific architectures, ELMo\\r\\nadvances the state of the art for several major NLP\\r\\nbenchmarks (Peters et al., 2018a) including ques\\x02tion answering (Rajpurkar et al., 2016), sentiment\\r\\nanalysis (Socher et al., 2013), and named entity\\r\\nrecognition (Tjong Kim Sang and De Meulder,\\r\\n2003). Melamud et al. (2016) proposed learning\\r\\ncontextual representations through a task to pre\\x02dict a single word from both left and right context\\r\\nusing LSTMs. Similar to ELMo, their model is\\r\\nfeature-based and not deeply bidirectional. Fedus\\r\\net al. (2018) shows that the cloze task can be used\\r\\nto improve the robustness of text generation mod\\x02els.\\r\\n2.2 Unsupervised Fine-tuning Approaches\\r\\nAs with the feature-based approaches, the first\\r\\nworks in this direction only pre-trained word em\\x02bedding parameters from unlabeled text (Col\\x02lobert and Weston, 2008).\\r\\nMore recently, sentence or document encoders\\r\\nwhich produce contextual token representations\\r\\nhave been pre-trained from unlabeled text and\\r\\nfine-tuned for a supervised downstream task (Dai\\r\\nand Le, 2015; Howard and Ruder, 2018; Radford\\r\\net al., 2018). The advantage of these approaches\\r\\nis that few parameters need to be learned from\\r\\nscratch. At least partly due to this advantage,\\r\\nOpenAI GPT (Radford et al., 2018) achieved pre\\x02viously state-of-the-art results on many sentence\\x02level tasks from the GLUE benchmark (Wang\\r\\net al., 2018a). Left-to-right language model-\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 2}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used\\r\\nfor pre-training such models (Howard and Ruder,\\r\\n2018; Radford et al., 2018; Dai and Le, 2015).\\r\\n2.3 Transfer Learning from Supervised Data\\r\\nThere has also been work showing effective trans\\x02fer from supervised tasks with large datasets, such\\r\\nas natural language inference (Conneau et al.,\\r\\n2017) and machine translation (McCann et al.,\\r\\n2017). Computer vision research has also demon\\x02strated the importance of transfer learning from\\r\\nlarge pre-trained models, where an effective recipe\\r\\nis to fine-tune models pre-trained with Ima\\x02geNet (Deng et al., 2009; Yosinski et al., 2014).\\r\\n3 BERT\\r\\nWe introduce BERT and its detailed implementa\\x02tion in this section. There are two steps in our\\r\\nframework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\r\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\r\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\r\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\r\\nquestion-answering example in Figure 1 will serve\\r\\nas a running example for this section.\\r\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\r\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\r\\nthe tensor2tensor library.1 Because the use\\r\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\r\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\r\\nVaswani et al. (2017) as well as excellent guides\\r\\nsuch as “The Annotated Transformer.”2\\r\\nIn this work, we denote the number of layers\\r\\n(i.e., Transformer blocks) as L, the hidden size as\\r\\nH, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\r\\nA=16, Total Parameters=340M).\\r\\nBERTBASE was chosen to have the same model\\r\\nsize as OpenAI GPT for comparison purposes.\\r\\nCritically, however, the BERT Transformer uses\\r\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\r\\ntoken can only attend to context to its left.4\\r\\n1\\r\\nhttps://github.com/tensorflow/tensor2tensor\\r\\n2\\r\\nhttp://nlp.seas.harvard.edu/2018/04/03/attention.html\\r\\n3\\r\\nIn all cases we set the feed-forward/filter size to be 4H,\\r\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\r\\n4We note that in the literature the bidirectional Trans-\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 3}, page_content='4174\\r\\nInput/Output Representations To make BERT\\r\\nhandle a variety of down-stream tasks, our input\\r\\nrepresentation is able to unambiguously represent\\r\\nboth a single sentence and a pair of sentences\\r\\n(e.g., h Question, Answeri) in one token sequence.\\r\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\r\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\r\\nWe use WordPiece embeddings (Wu et al.,\\r\\n2016) with a 30,000 token vocabulary. The first\\r\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\r\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\r\\ntasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special\\r\\ntoken ([SEP]). Second, we add a learned embed\\x02ding to every token indicating whether it belongs\\r\\nto sentence A or sentence B. As shown in Figure 1,\\r\\nwe denote input embedding as E, the final hidden\\r\\nvector of the special [CLS] token as C ∈ R\\r\\nH,\\r\\nand the final hidden vector for the i\\r\\nth input token\\r\\nas Ti ∈ R\\r\\nH.\\r\\nFor a given token, its input representation is\\r\\nconstructed by summing the corresponding token,\\r\\nsegment, and position embeddings. A visualiza\\x02tion of this construction can be seen in Figure 2.\\r\\n3.1 Pre-training BERT\\r\\nUnlike Peters et al. (2018a) and Radford et al.\\r\\n(2018), we do not use traditional left-to-right or\\r\\nright-to-left language models to pre-train BERT.\\r\\nInstead, we pre-train BERT using two unsuper\\x02vised tasks, described in this section. This step\\r\\nis presented in the left part of Figure 1.\\r\\nTask #1: Masked LM Intuitively, it is reason\\x02able to believe that a deep bidirectional model is\\r\\nstrictly more powerful than either a left-to-right\\r\\nmodel or the shallow concatenation of a left-to\\x02right and a right-to-left model. Unfortunately,\\r\\nstandard conditional language models can only be\\r\\ntrained left-to-right or right-to-left, since bidirec\\x02tional conditioning would allow each word to in\\x02directly “see itself”, and the model could trivially\\r\\npredict the target word in a multi-layered context.\\r\\nformer is often referred to as a “Transformer encoder” while\\r\\nthe left-context-only version is referred to as a “Transformer\\r\\ndecoder” since it can be used for text generation.\\r\\nIn order to train a deep bidirectional representa\\x02tion, we simply mask some percentage of the input\\r\\ntokens at random, and then predict those masked\\r\\ntokens. We refer to this procedure as a “masked\\r\\nLM” (MLM), although it is often referred to as a\\r\\nCloze task in the literature (Taylor, 1953). In this\\r\\ncase, the final hidden vectors corresponding to the\\r\\nmask tokens are fed into an output softmax over\\r\\nthe vocabulary, as in a standard LM. In all of our\\r\\nexperiments, we mask 15% of all WordPiece to\\x02kens in each sequence at random. In contrast to\\r\\ndenoising auto-encoders (Vincent et al., 2008), we\\r\\nonly predict the masked words rather than recon\\x02structing the entire input.\\r\\nAlthough this allows us to obtain a bidirec\\x02tional pre-trained model, a downside is that we\\r\\nare creating a mismatch between pre-training and\\r\\nfine-tuning, since the [MASK] token does not ap\\x02pear during fine-tuning. To mitigate this, we do\\r\\nnot always replace “masked” words with the ac\\x02tual [MASK] token. The training data generator\\r\\nchooses 15% of the token positions at random for\\r\\nprediction. If the i-th token is chosen, we replace\\r\\nthe i-th token with (1) the [MASK] token 80% of\\r\\nthe time (2) a random token 10% of the time (3)\\r\\nthe unchanged i-th token 10% of the time. Then,\\r\\nTi will be used to predict the original token with\\r\\ncross entropy loss. We compare variations of this\\r\\nprocedure in Appendix C.2.\\r\\nTask #2: Next Sentence Prediction (NSP)\\r\\nMany important downstream tasks such as Ques\\x02tion Answering (QA) and Natural Language Infer\\x02ence (NLI) are based on understanding the rela\\x02tionship between two sentences, which is not di\\x02rectly captured by language modeling. In order\\r\\nto train a model that understands sentence rela\\x02tionships, we pre-train for a binarized next sen\\x02tence prediction task that can be trivially gener\\x02ated from any monolingual corpus. Specifically,\\r\\nwhen choosing the sentences A and B for each pre\\x02training example, 50% of the time B is the actual\\r\\nnext sentence that follows A (labeled as IsNext),\\r\\nand 50% of the time it is a random sentence from\\r\\nthe corpus (labeled as NotNext). As we show\\r\\nin Figure 1, C is used for next sentence predic\\x02tion (NSP).5 Despite its simplicity, we demon\\x02strate in Section 5.1 that pre-training towards this\\r\\ntask is very beneficial to both QA and NLI. 6\\r\\n5The final model achieves 97%-98% accuracy on NSP.\\r\\n6The vector C is not a meaningful sentence representation\\r\\nwithout fine-tuning, since it was trained with NSP.\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 4}, page_content='4175\\r\\n[CLS] my dog is cute [SEP] he likes play ##ing [SEP] Input\\r\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP] Emy Edog Eis Ecute E[SEP]\\r\\nToken\\r\\nEmbeddings\\r\\nEAEBEBEBEBEBEAEAEAEAEA\\r\\nSegment\\r\\nEmbeddings\\r\\nE0 E6E7E8E9E10 E1E2E3E4E5\\r\\nPosition\\r\\nEmbeddings\\r\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta\\x02tion embeddings and the position embeddings.\\r\\nThe NSP task is closely related to representation\\x02learning objectives used in Jernite et al. (2017) and\\r\\nLogeswaran and Lee (2018). However, in prior\\r\\nwork, only sentence embeddings are transferred to\\r\\ndown-stream tasks, where BERT transfers all pa\\x02rameters to initialize end-task model parameters.\\r\\nPre-training data The pre-training procedure\\r\\nlargely follows the existing literature on language\\r\\nmodel pre-training. For the pre-training corpus we\\r\\nuse the BooksCorpus (800M words) (Zhu et al.,\\r\\n2015) and English Wikipedia (2,500M words).\\r\\nFor Wikipedia we extract only the text passages\\r\\nand ignore lists, tables, and headers. It is criti\\x02cal to use a document-level corpus rather than a\\r\\nshuffled sentence-level corpus such as the Billion\\r\\nWord Benchmark (Chelba et al., 2013) in order to\\r\\nextract long contiguous sequences.\\r\\n3.2 Fine-tuning BERT\\r\\nFine-tuning is straightforward since the self\\x02attention mechanism in the Transformer al\\x02lows BERT to model many downstream tasks—\\r\\nwhether they involve single text or text pairs—by\\r\\nswapping out the appropriate inputs and outputs.\\r\\nFor applications involving text pairs, a common\\r\\npattern is to independently encode text pairs be\\x02fore applying bidirectional cross attention, such\\r\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\r\\ninstead uses the self-attention mechanism to unify\\r\\nthese two stages, as encoding a concatenated text\\r\\npair with self-attention effectively includes bidi\\x02rectional cross attention between two sentences.\\r\\nFor each task, we simply plug in the task\\x02specific inputs and outputs into BERT and fine\\x02tune all the parameters end-to-end. At the in\\x02put, sentence A and sentence B from pre-training\\r\\nare analogous to (1) sentence pairs in paraphras\\x02ing, (2) hypothesis-premise pairs in entailment, (3)\\r\\nquestion-passage pairs in question answering, and\\r\\n(4) a degenerate text-∅ pair in text classification\\r\\nor sequence tagging. At the output, the token rep\\x02resentations are fed into an output layer for token\\x02level tasks, such as sequence tagging or question\\r\\nanswering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama:\n",
        "\n",
        "pdf[:5], PDF dosyasının ilk beş sayfasını seçer.\n",
        "# first five pages (ilk beş sayfa) ifadesi bir yorumdur ve kodun ne yaptığını açıklamak için kullanılır.\n",
        "Amaç:\n",
        "\n",
        "PDF dosyasının sadece ilk beş sayfasının içeriğini almak ve bu içerik üzerinde çalışmak."
      ],
      "metadata": {
        "id": "l6d_YeEHyhJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod parçaları, bir PDF dosyasının ilk beş sayfasının metin içeriğini alır, bu metinleri uygun büyüklükteki parçalara böler ve ardından OpenAI dil modelini kullanarak özetleme işlemi gerçekleştirir. Bu işlem, kullanıcının uzun belgelerden daha kısa ve anlamlı özetler almasına yardımcı olur."
      ],
      "metadata": {
        "id": "ya9DYqZEymQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Splitter"
      ],
      "metadata": {
        "id": "JvrLsoivTulb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc=chunk_data(docs=pdf)\n",
        "len(pdf_doc)"
      ],
      "metadata": {
        "id": "5j9NMbSCbMyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd95a9f-f328-4abd-9b97-d228ae40ba60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Açıklama:\n",
        "\n",
        "chunk_data fonksiyonu, PDF dosyasının içeriğini (metinlerini) alır ve onları daha küçük parçalar (chunk'lar) halinde böler.\n",
        "docs=pdf parametresi, bu fonksiyonun pdf değişkenindeki metinleri işleyip parçalara ayırması gerektiğini belirtir. pdf değişkeni, daha önce PDF dosyasının içeriğini tutmak için kullanılmıştı.\n",
        "Amaç:\n",
        "\n",
        "Uzun metinleri veya belgeleri, dil modelinin daha iyi işleyebileceği daha küçük parçalara ayırmak. Bu, performansı artırır ve modelin daha anlamlı özetler veya yanıtlar oluşturmasına olanak tanır."
      ],
      "metadata": {
        "id": "JR3tQOeOy9K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " len(pdf_doc)\n",
        "Açıklama:\n",
        "\n",
        "len(pdf_doc) ifadesi, pdf_doc değişkenindeki parçaların (chunk'ların) sayısını hesaplar.\n",
        "pdf_doc değişkeni, chunk_data fonksiyonu tarafından oluşturulan ve PDF'in parçalarını içeren bir liste olarak kabul edilir.\n",
        "Amaç:\n",
        "\n",
        "PDF dosyasının kaç parçaya (chunk'a) bölündüğünü kontrol etmek. Bu, parçaların sayısına göre modelin özetleme işlemlerini optimize etmek için önemlidir."
      ],
      "metadata": {
        "id": "3OBJ1TD3y_sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Belgeyi parçalara ayırın\n",
        "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
        "#docs = text_splitter.split_documents(pdf)"
      ],
      "metadata": {
        "id": "53mpwb7KbMrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)"
      ],
      "metadata": {
        "id": "r0PsvQRsFVbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " from langchain_openai import ChatOpenAI\n",
        "Açıklama:\n",
        "\n",
        "ChatOpenAI sınıfı, OpenAI'nin dil modellerine (örneğin GPT-4) erişmek ve onları kullanmak için bir arayüz sağlar.\n",
        "Bu sınıf, LangChain'in bir parçasıdır ve OpenAI API'sine bağlanarak dil işleme görevlerini gerçekleştirmek için kullanılır.\n",
        "Amaç:\n",
        "\n",
        "OpenAI'nin güçlü dil modelini (gpt-4o-mini gibi) özetleme görevinde kullanmak."
      ],
      "metadata": {
        "id": "znQdmBFSzDiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". from langchain.chains.summarize import load_summarize_chain\n",
        "Açıklama:\n",
        "\n",
        "load_summarize_chain fonksiyonu, bir özetleme zinciri oluşturur.\n",
        "Bu zincir, belirli bir metni özetlemek için kullanılır. Kullanıcının sağladığı metin parçalarını işleyerek, kısa ve anlamlı özetler üretir.\n",
        "Amaç:\n",
        "\n",
        "Dil modelini kullanarak, uzun metinleri veya belgeleri otomatik olarak özetleyen bir süreç oluşturmak."
      ],
      "metadata": {
        "id": "VcdK0JwKzFds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". llm = ChatOpenAI(temperature=0, model_name='gpt-4o-mini', max_tokens=1024)\n",
        "Açıklama:\n",
        "\n",
        "ChatOpenAI sınıfı, OpenAI'nin gpt-4o-mini modelini kullanarak bir dil modeli nesnesi oluşturur.\n",
        "temperature=0: Bu, modelin yanıtlarının deterministik (öngörülebilir) olmasını sağlar, yani aynı girdiye her seferinde benzer bir çıktı üretilir.\n",
        "max_tokens=1024: Bu, modelin üretebileceği maksimum token sayısını (kelime ve semboller) belirtir. Yani, yanıtın uzunluğu bu sınırı aşmaz.\n",
        "Amaç:\n",
        "\n",
        "gpt-4o-mini modelini kullanarak, dil işleme görevlerini gerçekleştirmek ve özetleme zincirini beslemek için bir dil modeli örneği oluşturmak."
      ],
      "metadata": {
        "id": "99G7p8KpzHxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type='stuff'\n",
        ")\n",
        "output_summary = chain.invoke(pdf_doc[0:5])['output_text']"
      ],
      "metadata": {
        "id": "B6mNWgRAFY6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain = load_summarize_chain(llm, chain_type='stuff')\n",
        "Açıklama:\n",
        "\n",
        "load_summarize_chain fonksiyonu kullanılarak bir özetleme zinciri (chain) oluşturulur.\n",
        "llm: Daha önce tanımlanan ChatOpenAI nesnesini belirtir. Bu, zincirin hangi dil modelini kullanacağını tanımlar.\n",
        "chain_type='stuff': stuff zincir türü, verilen tüm metni tek bir seferde işlemek için kullanılır.\n",
        "Amaç:\n",
        "\n",
        "Belirli bir dil modeli kullanarak (gpt-4o-mini), metinleri özetlemek için bir zincir oluşturmak."
      ],
      "metadata": {
        "id": "AyOBO75szNq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " output_summary = chain.invoke(pdf_doc[0:5])['output_text']\n",
        "Açıklama:\n",
        "\n",
        "chain.invoke(pdf_doc[0:5]): Özetleme zincirini çağırır (invoke), ve pdf_doc değişkeninin ilk beş parçasını işleyerek bir özet oluşturur.\n",
        "pdf_doc[0:5]: PDF'in ilk beş parçasını (chunk'ını) alır.\n",
        "['output_text']: invoke fonksiyonunun çıktısındaki özet metni alır.\n",
        "Amaç:\n",
        "\n",
        "PDF'in ilk beş parçası üzerinde bir özetleme işlemi gerçekleştirmek ve çıkan özeti output_summary değişkenine kaydetmek."
      ],
      "metadata": {
        "id": "bOv0mlatzRNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "lf0QW7u2FdU0",
        "outputId": "171927e6-d704-4ba9-de27-2843637146f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. Unlike previous models that use unidirectional context, BERT pre-trains deep bidirectional representations from unlabeled text, allowing it to consider both left and right context in all layers. This design enables BERT to be fine-tuned with minimal additional architecture for various natural language processing tasks, achieving state-of-the-art results on eleven benchmarks, including significant improvements in GLUE, MultiNLI, and SQuAD. BERT employs a masked language model pre-training objective, enhancing its effectiveness for both sentence-level and token-level tasks."
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from IPython.display import Markdown\n",
        "Açıklama:\n",
        "\n",
        "IPython kütüphanesinin Markdown sınıfı, Jupyter Notebook veya Google Colab ortamında Markdown formatında metin göstermek için kullanılır.\n",
        "Amaç:\n",
        "\n",
        "Özetlenen metni Markdown formatında daha güzel ve okunabilir bir şekilde göstermek."
      ],
      "metadata": {
        "id": "eEAzZIzgzUuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markdown(output_summary)\n",
        "Açıklama:\n",
        "\n",
        "output_summary değişkenindeki özet metni, Markdown formatında görüntülenir.\n",
        "Amaç:\n",
        "\n",
        "Özet metnin kullanıcıya okunabilir ve düzgün formatta gösterilmesini sağlamak."
      ],
      "metadata": {
        "id": "299q92eDzcfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özet\n",
        "Bu kod, bir PDF dosyasının ilk beş parçasını alır, bu parçaları küçük segmentlere böler, ardından OpenAI'nin dil modelini kullanarak bu segmentlerin özetini çıkarır ve özet metni Markdown formatında görüntüler.\n",
        "\n",
        "Adım Adım İşleyiş:\n",
        "\n",
        "PDF Dosyasını Bölme: PDF içeriği, daha küçük parçalara bölünür.\n",
        "Dil Modeli Tanımlama: gpt-4o-mini modeli kullanılarak özetleme işlemi için bir dil modeli oluşturulur.\n",
        "Özetleme Zinciri Oluşturma: Dil modeli, stuff tipi bir özetleme zincirine entegre edilir.\n",
        "Özetleme İşlemi: İlk beş parçadan özet çıkarılır ve output_summary değişkeninde saklanır.\n",
        "Sonucun Gösterilmesi: Özetlenen metin Markdown formatında kullanıcıya gösterilir.\n",
        "Bu süreç, kullanıcıya uzun bir PDF belgesinin özetini otomatik olarak çıkartarak, daha kısa ve anlaşılır bir metin sunar."
      ],
      "metadata": {
        "id": "Mcy4UPkSzdwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make A Brief Summary of The Entire Document With Chain_Types of \"map_reduce\" and \"refine\""
      ],
      "metadata": {
        "id": "3zlVe2iISX0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**chain_type = map_reduce**"
      ],
      "metadata": {
        "id": "vQkcTpuTFlPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)"
      ],
      "metadata": {
        "id": "t6wPW3OFbOcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerekli Modüllerin Yüklenmesi:\n",
        "\n",
        "load_summarize_chain: Metin özetleme zincirini yüklemek için kullanılır.\n",
        "textwrap: Metinlerin belirli bir genişlikte sarılması için kullanılabilir, ancak bu kodda kullanılmamış.\n",
        "ChatOpenAI: OpenAI dil modeliyle iletişim kurmak için kullanılır."
      ],
      "metadata": {
        "id": "CKSbiAV00nvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM (Dil Modeli) Tanımlaması:\n",
        "\n",
        "llm adlı değişken, OpenAI'nin dil modelini çalıştırmak için yapılandırılmıştır. temperature=0, çıktının daha deterministik olmasını sağlar. model_name='gpt-4o-mini' modeli kullanılıyor ve maksimum 1024 tokenlık cevaplar alınıyor."
      ],
      "metadata": {
        "id": "kL1D5UJr0oT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IaakAyAA0rPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
        "chunks = text_splitter.split_documents(pdf)"
      ],
      "metadata": {
        "id": "8NpJYGxRbOVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metin Bölme:\n",
        "\n",
        "RecursiveCharacterTextSplitter kullanılarak, uzun bir metin belirli parçalara ayrılıyor. Bu, çok büyük bir metni yönetilebilir parçalara ayırarak özetleme işlemini kolaylaştırır.\n",
        "chunk_size=10000, her bir parçanın maksimum 10.000 karakter olmasını sağlar. chunk_overlap=0 ise parçalar arasında örtüşme olmadığını belirtir.\n",
        "chunks değişkeni, bu bölünmüş metin parçalarını tutar ve len(chunks) parçaların sayısını döndürür."
      ],
      "metadata": {
        "id": "OtrbiLPQ0t1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu satır, langchain kütüphanesindeki metin bölme işlemleri için kullanılan iki sınıfı (RecursiveCharacterTextSplitter ve CharacterTextSplitter) içe aktarır.\n",
        "Not: Bu örnekte sadece RecursiveCharacterTextSplitter kullanılıyor. CharacterTextSplitter burada yedek bir seçenek olarak bulunuyor."
      ],
      "metadata": {
        "id": "PYgxCfAI11gL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu satır, RecursiveCharacterTextSplitter sınıfından bir nesne oluşturur.\n",
        "Parametreler:\n",
        "chunk_size=10000: Her bir metin parçasının maksimum uzunluğunu 10.000 karakter olarak belirler. Yani, metin parçalara bölünürken her parça en fazla 10.000 karakter uzunluğunda olacaktır.\n",
        "chunk_overlap=0: Metin parçaları arasında hiçbir örtüşme olmamasını sağlar. Yani, bir parça bittiği yerde diğeri başlar; parçalar arasında paylaşılan karakterler yoktur.\n",
        "RecursiveCharacterTextSplitter: Bu sınıf, metinleri belirli kurallara göre (örneğin, cümle sınırları, paragraflar, vb.) bölme yeteneğine sahiptir. Bu sayede metinler anlamlı bir şekilde parçalara ayrılır."
      ],
      "metadata": {
        "id": "8fJTY5t114VA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu satır, pdf adlı dokümanı/metni text_splitter nesnesi aracılığıyla parçalara böler.\n",
        "Ne Yapar:\n",
        "split_documents(pdf): Bu metodu çağırarak, pdf adlı doküman/metin belirlenen chunk_size ve chunk_overlap parametrelerine göre bölünür. Sonuçta, her biri maksimum 10.000 karakter uzunluğunda olan metin parçaları elde edilir.\n",
        "chunks: Bu değişken, bölünmüş metin parçalarının bir listesini tutar. Yani, chunks bir listedir ve her bir elemanı metnin bir parçasını temsil eder."
      ],
      "metadata": {
        "id": "de8jIy0g16pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1uHb1M0Fu2G",
        "outputId": "cfd4a47f-5049-451a-aed6-3bbf8d25b901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "len(chunks)\n",
        "Anlamı: Bu ifade, chunks adlı listenin uzunluğunu, yani metnin kaç parçaya bölündüğünü gösterir.\n",
        "Ne Yapar: text_splitter.split_documents(pdf) ile metin (veya doküman) belirli boyutlarda parçalara bölündü. len(chunks) ifadesi ise bu parçalardan oluşan listenin eleman sayısını döndürür.\n",
        "Sonuç: Eğer çıktı olarak 5 gibi bir değer alırsanız, bu metnin 5 parçaya bölündüğü anlamına gelir."
      ],
      "metadata": {
        "id": "t3sJtTYX1Y7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbZuh-pFFx_U",
        "outputId": "6b3d9967-d959-477f-af3d-e7c032186959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/Colab Notebooks/Capstone/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunks[0]\n",
        "Anlamı: Bu ifade, chunks listesindeki ilk parçayı (yani birinci metin parçasını) alır.\n",
        "Ne Yapar: chunks listesi içinde birçok metin parçası vardır. chunks[0] bu parçaların ilkiyle ilgilidir ve ilk metin parçasını görüntülemeye yarar.\n",
        "Sonuç: Bu, bölünen metnin ilk parçasının içeriğini gösterir. Örneğin, bu parçada metnin ilk 10.000 karakterlik kısmı bulunabilir."
      ],
      "metadata": {
        "id": "C9t7SW001ayt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\")\n",
        "\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "la6cz1pTFx7Y",
        "outputId": "0eb81fe9-d2a2-45ee-cb4a-741e3b39b159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 913 ms, sys: 93.7 ms, total: 1.01 s\n",
            "Wall time: 34.9 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper presents BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model developed by Google AI Language. BERT utilizes deep bidirectional representations, allowing it to consider both left and right context during pre-training on unlabeled text, which enhances its performance on various natural language processing (NLP) tasks. It employs a masked language model (MLM) and next sentence prediction (NSP) for pre-training, enabling effective fine-tuning with minimal task-specific adjustments. BERT achieves state-of-the-art results on eleven NLP benchmarks, including GLUE and SQuAD, outperforming previous models like OpenAI GPT and ELMo. The model's architecture includes a multi-layer bidirectional Transformer encoder, and it supports versatile input representations for single and paired sentences. The study highlights the significance of extensive pre-training, model size, and effective masking strategies in optimizing performance across diverse NLP tasks. BERT's code and pre-trained models are publicly available, contributing to ongoing advancements in the field."
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Map-Reduce\" Zincir Tipi ile Özetleme:\n",
        "\n",
        "chain_type=\"map_reduce\" kullanılarak, parçalar üzerinde bir \"map-reduce\" yöntemiyle özetleme yapılır. Bu yöntemde önce her bir parça özetlenir, ardından bu özetler birleştirilerek son özet oluşturulur.\n",
        "output_summary değişkeni bu işlemin sonucundaki özeti tutar ve Markdown(output_summary) ile bu özet Markdown formatında görüntülenir."
      ],
      "metadata": {
        "id": "o4fB_Jzg0ya0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**chain_type = refine**"
      ],
      "metadata": {
        "id": "sWnOyKbqF5Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXC8XfxlF2Z-",
        "outputId": "aa9b2dc0-24e5-4da6-d1ee-5e04b5c34866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 536 ms, sys: 61.1 ms, total: 597 ms\n",
            "Wall time: 1min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Refine\" Zincir Tipi ile Özetleme:\n",
        "\n",
        "chain_type=\"refine\" ile yapılan bu özetleme yöntemi, önce her parçayı özetler, sonra bu özetleri iteratif olarak rafine ederek son özeti oluşturur.\n",
        "Bu işlem sonucunda output_summary değişkeni nihai özeti tutar ve Markdown(output_summary) ile bu özet görüntülenir."
      ],
      "metadata": {
        "id": "fSeVeg2a01vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zamanlama:\n",
        "\n",
        "%%time komutları, her özetleme işleminin ne kadar sürdüğünü zamanlamak için kullanılır.\n",
        "Bu işlem, uzun belgeleri veya metinleri özetlemek için kullanılır ve map_reduce ve refine gibi farklı stratejilerle çalışabilir. Her iki yöntemin de avantajları ve kullanım durumları vardır."
      ],
      "metadata": {
        "id": "O63mImvH04BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnHQUsNiF2Md",
        "outputId": "49ebf3b9-1ca8-4e83-def2-39c47ccd4b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RefineDocumentsChain(initial_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ec3d94019c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ec3dbf56230>, root_client=<openai.OpenAI object at 0x7ec3de87b610>, root_async_client=<openai.AsyncOpenAI object at 0x7ec3d94019f0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), refine_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['existing_answer', 'text'], template=\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ec3d94019c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ec3dbf56230>, root_client=<openai.OpenAI object at 0x7ec3de87b610>, root_async_client=<openai.AsyncOpenAI object at 0x7ec3d94019f0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text', initial_response_name='existing_answer')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain\n",
        "Anlamı: Bu ifade, özetleme zinciri nesnesini (chain) temsil eder.\n",
        "Ne Yapar:\n",
        "Daha önceki adımlarda load_summarize_chain fonksiyonu ile bir özetleme zinciri oluşturulmuştu. chain bu oluşturulan zinciri ifade eder.\n",
        "Bu nesne, metni özetlemek için kullanılır ve hangi stratejinin (map_reduce veya refine) kullanıldığını belirtir.\n",
        "Tek başına chain yazılması, genellikle bu nesnenin durumunu veya yapılandırmasını görüntülemek için kullanılır."
      ],
      "metadata": {
        "id": "UXBNDAho2Whu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "fuw1tE8-GSQC",
        "outputId": "f0eddfe5-4960-415a-dfc1-21ae726f9c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. Unlike previous models that use unidirectional context, BERT pre-trains deep bidirectional representations from unlabeled text, allowing it to consider both left and right context in all layers. This design enables BERT to be fine-tuned with minimal additional architecture for various natural language processing tasks, achieving state-of-the-art results on eleven benchmarks, including significant improvements in GLUE, MultiNLI, and SQuAD tasks.\n\nBERT employs a masked language model (MLM) pre-training objective, which enhances its effectiveness for both sentence-level and token-level tasks. The model randomly masks a percentage of input tokens and predicts them, allowing for a deep bidirectional representation. Specifically, during the masking procedure, 80% of the time a token is replaced with a [MASK] token, 10% of the time it is replaced with a random word, and 10% of the time it remains unchanged. This approach ensures that the Transformer encoder maintains a distributional contextual representation of every input token. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task that trains the model to understand the relationship between sentence pairs, which is crucial for tasks like Question Answering (QA) and Natural Language Inference (NLI). The NSP task is designed to transfer all parameters to initialize end-task model parameters, unlike prior work that only transferred sentence embeddings.\n\nTo handle a variety of downstream tasks, BERT's input representation can unambiguously represent both single sentences and pairs of sentences in one token sequence. It uses WordPiece embeddings with a 30,000 token vocabulary, where the first token of every sequence is a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. The input embeddings are the sum of token embeddings, segment embeddings, and position embeddings, allowing for effective modeling of text pairs through a unified self-attention mechanism.\n\nBERT's architecture is a multi-layer bidirectional Transformer encoder, with two primary model sizes: BERTBASE and BERTLARGE. The pre-training corpus includes the BooksCorpus (800M words) and English Wikipedia (2,500M words), emphasizing the importance of using a document-level corpus to extract long contiguous sequences. Fine-tuning BERT is straightforward and can be accomplished in a relatively short time, with results on various tasks replicable in under an hour on a single Cloud TPU.\n\nBERT marks a significant advancement in the field of NLP by reducing the need for heavily-engineered task-specific architectures and achieving state-of-the-art performance across a wide range of tasks. In particular, BERTBASE and BERTLARGE outperform all previous systems on the GLUE benchmark, with BERTLARGE achieving an average accuracy improvement of 7.0% over the prior state of the art. On the SQuAD v1.1 dataset, BERT demonstrates superior performance, surpassing top leaderboard systems by notable margins, including achieving an F1 score of 90.9 with BERTLARGE and 91.8 with an ensemble approach. In the SQuAD v2.0 task, BERT also shows a +5.1 F1 improvement over the previous best system, demonstrating its robustness in handling more complex question-answering scenarios. Additionally, BERTLARGE outperforms other models on the SWAG dataset, showcasing its effectiveness in grounded common-sense inference tasks.\n\nAblation studies reveal the importance of the NSP task, as removing it significantly degrades performance on tasks like QNLI, MNLI, and SQuAD. Furthermore, the model size plays a crucial role in performance, with larger models consistently achieving better accuracy across various tasks, even those with limited training data. BERTBASE contains 110M parameters, while BERTLARGE has 340M parameters, demonstrating that scaling model size leads to substantial improvements in performance.\n\nThe paper also explores the effectiveness of both fine-tuning and feature-based approaches using BERT. While the fine-tuning approach, where a simple classification layer is added to the pre-trained model, has shown significant success, the feature-based approach—where fixed features are extracted from the pre-trained model—offers computational benefits and can be advantageous for certain tasks that require task-specific architectures. Experiments on the CoNLL-2003 Named Entity Recognition task indicate that BERT can perform competitively using both methods, with the feature-based approach yielding results close to those achieved through fine-tuning.\n\nAdditional ablation studies further investigate the impact of the number of training steps and different masking strategies on BERT's performance. Results indicate that BERT benefits from extensive pre-training, with accuracy improving significantly with more training steps. The studies also show that while fine-tuning is robust to various masking strategies, the feature-based approach is more sensitive to the specific masking used during pre-training. Overall, BERT's ability to generalize across various NLP tasks through rich, unsupervised pre-training marks"
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markdown(output_summary)\n",
        "Anlamı: Bu ifade, özetleme sonucunu Markdown formatında görüntülemek için kullanılır.\n",
        "Ne Yapar:\n",
        "output_summary değişkeni, metin özetleme zincirinin (chain) çıktı olarak ürettiği özeti tutar.\n",
        "Markdown(output_summary) ifadesi, bu özeti Markdown formatında görüntülemeye yarar. Markdown, metinleri zenginleştirilmiş bir şekilde (örneğin, başlıklar, listeler, kalın yazı vb.) görüntülemek için kullanılan bir biçimlendirme dilidir.\n",
        "Bu sayede, özet sonucu daha okunabilir ve biçimlendirilmiş bir şekilde gösterilir."
      ],
      "metadata": {
        "id": "-JJTooib2aTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Özet\n",
        "chain ifadesi, özetleme işlemini gerçekleştiren zinciri temsil eder.\n",
        "Markdown(output_summary) ise bu zincirin oluşturduğu özeti Markdown formatında görüntülemeye yarar."
      ],
      "metadata": {
        "id": "M1X9ax482cUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate A Detailed Summary of The Entire Document With At Least 1000 Tokens. Also, Add A Title To The Summary And Present Key Points Using Bullet Points With Chain_Type of \"map_reduce\"."
      ],
      "metadata": {
        "id": "9zZxse-ZUV3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**map_reduce with custom prompt**"
      ],
      "metadata": {
        "id": "0cVLJF2SGYyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce'\n",
        ")\n",
        "chain"
      ],
      "metadata": {
        "id": "lpuiEedJbQME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65556e0-9276-4335-83e5-53ff9b62782d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ec3d94019c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ec3dbf56230>, root_client=<openai.OpenAI object at 0x7ec3de87b610>, root_async_client=<openai.AsyncOpenAI object at 0x7ec3d94019f0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ec3d94019c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ec3dbf56230>, root_client=<openai.OpenAI object at 0x7ec3de87b610>, root_async_client=<openai.AsyncOpenAI object at 0x7ec3d94019f0>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu satır, özetleme işlemini gerçekleştirecek olan bir zincir (chain) oluşturur.\n",
        "Parametreler:\n",
        "llm=llm: Önceden tanımlanmış dil modelini (llm) kullanır.\n",
        "chain_type='map_reduce': Map-Reduce yöntemiyle özetleme yapılacağını belirtir. Bu yöntemde önce her metin parçası özetlenir, sonra bu özetler birleştirilerek nihai özet oluşturulur."
      ],
      "metadata": {
        "id": "4s0Ivi9w3XWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for every chunk\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Map prompt (her parça için kullanılacak prompt)\n",
        "chunks_prompt = \"\"\"\n",
        "Please summarize the following text:\n",
        "{text}\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(input_variables=['text'], template=chunks_prompt)"
      ],
      "metadata": {
        "id": "52w9w_SPGjYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu kısım, her bir metin parçası için kullanılacak olan özetleme talimatını (prompt) tanımlar.\n",
        "Ne Yapar:\n",
        "chunks_prompt: Her metin parçası için özetleme yapılacak talimatı içeren şablon.\n",
        "PromptTemplate: Bu sınıf, metni nasıl özetlemesi gerektiğini dil modeline söyleyen şablonları oluşturur.\n",
        "map_prompt_template: Bu, yukarıda tanımlanan şablonun (chunks_prompt) bir örneğidir."
      ],
      "metadata": {
        "id": "paMVd2n93f1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine prompt (bütün parçalar birleştirildiğinde kullanılacak prompt)\n",
        "final_combine_prompt = \"\"\"\n",
        "Summarize the text below with at least 1000 tokens. Provide a title and key points using bullet points:\n",
        "{text}\n",
        "\"\"\"\n",
        "final_combine_prompt_template = PromptTemplate(input_variables=['text'], template=final_combine_prompt)"
      ],
      "metadata": {
        "id": "OdFF9bOAbQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu kısım, bütün metin parçaları özetlendikten sonra, nihai özetin nasıl oluşturulacağını belirten talimatı tanımlar.\n",
        "Ne Yapar:\n",
        "final_combine_prompt: Nihai özet için en az 1000 token'lık bir özet oluşturulmasını ve başlık ile madde işaretleri kullanarak önemli noktaların vurgulanmasını ister.\n",
        "final_combine_prompt_template: Bu, nihai özetleme işlemi için kullanılacak olan şablondur."
      ],
      "metadata": {
        "id": "AZTHjXfI3j_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Map-Reduce zinciri oluşturma\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce',\n",
        "    map_prompt=map_prompt_template,\n",
        "    combine_prompt=final_combine_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "ASG41GGVGoO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu satır, yukarıda tanımlanan prompt şablonları ile bir Map-Reduce özetleme zinciri oluşturur.\n",
        "Ne Yapar:\n",
        "map_prompt=map_prompt_template: Her bir metin parçasını özetlemek için kullanılacak şablon.\n",
        "combine_prompt=final_combine_prompt_template: Tüm özetleri birleştirerek nihai özetin oluşturulacağı şablon."
      ],
      "metadata": {
        "id": "tOkRRh_i3m7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "output_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "F0YBFbAGGskb",
        "outputId": "96d1f60a-b180-40e4-a814-a5afb44f602b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Title: Advancements in Natural Language Processing: An In-Depth Overview of BERT\\n\\n## Key Points:\\n\\n- **Introduction to BERT**: \\n  - BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking language representation model developed by Google AI Language.\\n  - It is designed to pre-train deep bidirectional representations from unlabeled text, effectively considering both left and right context in all layers of the model.\\n\\n- **Pre-training and Fine-tuning**:\\n  - The architecture of BERT allows for efficient fine-tuning with minimal modifications for a variety of natural language processing (NLP) tasks.\\n  - BERT has achieved state-of-the-art results across eleven benchmarks, including GLUE (General Language Understanding Evaluation), MultiNLI (Multi-Genre Natural Language Inference), and SQuAD (Stanford Question Answering Dataset).\\n\\n- **Critique of Existing Models**:\\n  - The authors critique traditional pre-training strategies that rely on unidirectional models, which can limit effectiveness for certain NLP tasks.\\n  - BERT employs a \"masked language model\" (MLM) pre-training objective, which enhances contextual understanding by randomly masking tokens in the input text.\\n\\n- **Key Contributions of BERT**:\\n  - **Bidirectional Pre-training**: BERT utilizes MLM to create deep bidirectional representations, contrasting with previous unidirectional approaches that were less effective.\\n  - **Reduction of Task-Specific Architectures**: BERT minimizes the need for complex, task-specific models, achieving high performance through its fine-tuning capabilities.\\n  - **Performance Improvement**: BERT sets new benchmarks across various NLP tasks, with its code and pre-trained models made publicly available for further research and application.\\n\\n- **Evolution of Language Representation Models**:\\n  - The text reviews the evolution of language representation models, moving from traditional word embeddings to more sophisticated contextual models like ELMo and GPT.\\n  - These advanced models leverage contextual information from both left and right contexts, significantly improving performance on various NLP benchmarks.\\n\\n- **BERT\\'s Architecture**:\\n  - BERT consists of a multi-layer bidirectional Transformer encoder, which allows it to process input in both directions simultaneously.\\n  - The model comes in different sizes, including BERTBASE and BERTLARGE, with specific configurations and total parameters detailed.\\n\\n- **Input/Output Representations**:\\n  - BERT processes both single sentences and pairs of sentences using a unified token sequence.\\n  - It employs WordPiece embeddings with a vocabulary of 30,000 tokens, starting sequences with a special classification token ([CLS]) and differentiating sentence pairs with a separator token ([SEP]).\\n\\n- **Pre-training Tasks**:\\n  - **Masked Language Model (MLM)**: This task involves randomly masking a percentage of input tokens and training the model to predict them, which enhances its bidirectional representation capabilities.\\n  - **Next Sentence Prediction (NSP)**: This task trains the model to understand relationships between sentences by predicting whether a given sentence logically follows another.\\n\\n- **Fine-tuning Process**:\\n  - BERT is fine-tuned on labeled data for specific downstream tasks, using the final hidden vector from the input as an aggregate representation for classification tasks.\\n  - The fine-tuning process is efficient and requires minimal time on powerful hardware, making it accessible for various applications.\\n\\n- **Performance on Benchmarks**:\\n  - BERT models significantly outperform previous state-of-the-art systems on the GLUE benchmark and SQuAD v1.1.\\n  - BERTLARGE achieves an average accuracy improvement of 7.0% over prior models, excelling particularly in tasks with limited training data.\\n\\n- **SQuAD Results**:\\n  - BERTLARGE achieves an impressive F1 score of 91.8 on SQuAD 1.1, outperforming other models in the field.\\n  - For SQuAD 2.0, BERTLARGE shows an F1 score of 83.1, marking a 5.1 F1 improvement over the previous best system.\\n\\n- **SWAG Results**:\\n  - BERTLARGE excels in the SWAG (Situations With Adversarial Generations) dataset, achieving accuracy that surpasses previous models by substantial margins.\\n\\n- **Ablation Studies**:\\n  - Ongoing ablation studies aim to understand the importance of different components of the BERT model.\\n  - Removing the NSP task significantly degrades performance on tasks like QNLI (Question Natural Language Inference), MNLI, and SQuAD.\\n\\n- **Impact of Model Size**:\\n  - Increasing the model size enhances accuracy across various tasks, even for smaller datasets.\\n  - Larger models consistently yield better performance, demonstrating the importance of sufficient pre-training.\\n\\n- **Fine-tuning vs. Feature-based Approaches**:\\n  - The text compares the effectiveness of fine-tuning all parameters versus using fixed features extracted from the pre-trained model for tasks like Named Entity Recognition (NER).\\n  - The feature-based method'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu satır, metni özetlemek için oluşturulan zinciri kullanarak özetin çıkışını (output_summary) alır.\n",
        "Ne Yapar:\n",
        "chain.invoke(chunks): Bölünmüş metin parçalarını (chunks) özetlemek için zinciri çağırır.\n",
        "[\"output_text\"]: Zincirin oluşturduğu özet metnini alır ve output_summary değişkenine atar."
      ],
      "metadata": {
        "id": "wx0ZsnTw3p1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gpLko_SWGuxz",
        "outputId": "00fe092c-95e3-482b-8ffd-b46df1e660c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Title: Advancements in Natural Language Processing: An In-Depth Overview of BERT\n\n## Key Points:\n\n- **Introduction to BERT**: \n  - BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking language representation model developed by Google AI Language.\n  - It is designed to pre-train deep bidirectional representations from unlabeled text, effectively considering both left and right context in all layers of the model.\n\n- **Pre-training and Fine-tuning**:\n  - The architecture of BERT allows for efficient fine-tuning with minimal modifications for a variety of natural language processing (NLP) tasks.\n  - BERT has achieved state-of-the-art results across eleven benchmarks, including GLUE (General Language Understanding Evaluation), MultiNLI (Multi-Genre Natural Language Inference), and SQuAD (Stanford Question Answering Dataset).\n\n- **Critique of Existing Models**:\n  - The authors critique traditional pre-training strategies that rely on unidirectional models, which can limit effectiveness for certain NLP tasks.\n  - BERT employs a \"masked language model\" (MLM) pre-training objective, which enhances contextual understanding by randomly masking tokens in the input text.\n\n- **Key Contributions of BERT**:\n  - **Bidirectional Pre-training**: BERT utilizes MLM to create deep bidirectional representations, contrasting with previous unidirectional approaches that were less effective.\n  - **Reduction of Task-Specific Architectures**: BERT minimizes the need for complex, task-specific models, achieving high performance through its fine-tuning capabilities.\n  - **Performance Improvement**: BERT sets new benchmarks across various NLP tasks, with its code and pre-trained models made publicly available for further research and application.\n\n- **Evolution of Language Representation Models**:\n  - The text reviews the evolution of language representation models, moving from traditional word embeddings to more sophisticated contextual models like ELMo and GPT.\n  - These advanced models leverage contextual information from both left and right contexts, significantly improving performance on various NLP benchmarks.\n\n- **BERT's Architecture**:\n  - BERT consists of a multi-layer bidirectional Transformer encoder, which allows it to process input in both directions simultaneously.\n  - The model comes in different sizes, including BERTBASE and BERTLARGE, with specific configurations and total parameters detailed.\n\n- **Input/Output Representations**:\n  - BERT processes both single sentences and pairs of sentences using a unified token sequence.\n  - It employs WordPiece embeddings with a vocabulary of 30,000 tokens, starting sequences with a special classification token ([CLS]) and differentiating sentence pairs with a separator token ([SEP]).\n\n- **Pre-training Tasks**:\n  - **Masked Language Model (MLM)**: This task involves randomly masking a percentage of input tokens and training the model to predict them, which enhances its bidirectional representation capabilities.\n  - **Next Sentence Prediction (NSP)**: This task trains the model to understand relationships between sentences by predicting whether a given sentence logically follows another.\n\n- **Fine-tuning Process**:\n  - BERT is fine-tuned on labeled data for specific downstream tasks, using the final hidden vector from the input as an aggregate representation for classification tasks.\n  - The fine-tuning process is efficient and requires minimal time on powerful hardware, making it accessible for various applications.\n\n- **Performance on Benchmarks**:\n  - BERT models significantly outperform previous state-of-the-art systems on the GLUE benchmark and SQuAD v1.1.\n  - BERTLARGE achieves an average accuracy improvement of 7.0% over prior models, excelling particularly in tasks with limited training data.\n\n- **SQuAD Results**:\n  - BERTLARGE achieves an impressive F1 score of 91.8 on SQuAD 1.1, outperforming other models in the field.\n  - For SQuAD 2.0, BERTLARGE shows an F1 score of 83.1, marking a 5.1 F1 improvement over the previous best system.\n\n- **SWAG Results**:\n  - BERTLARGE excels in the SWAG (Situations With Adversarial Generations) dataset, achieving accuracy that surpasses previous models by substantial margins.\n\n- **Ablation Studies**:\n  - Ongoing ablation studies aim to understand the importance of different components of the BERT model.\n  - Removing the NSP task significantly degrades performance on tasks like QNLI (Question Natural Language Inference), MNLI, and SQuAD.\n\n- **Impact of Model Size**:\n  - Increasing the model size enhances accuracy across various tasks, even for smaller datasets.\n  - Larger models consistently yield better performance, demonstrating the importance of sufficient pre-training.\n\n- **Fine-tuning vs. Feature-based Approaches**:\n  - The text compares the effectiveness of fine-tuning all parameters versus using fixed features extracted from the pre-trained model for tasks like Named Entity Recognition (NER).\n  - The feature-based method"
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlamı: Bu kısım, özetin Markdown formatında görüntülenmesini sağlar.\n",
        "Ne Yapar:\n",
        "Markdown(output_summary): Özet metnini Markdown formatında düzenler ve görüntüler."
      ],
      "metadata": {
        "id": "m93sHDqz3tpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genel Özet\n",
        "Bu kod parçacığı, bir belgeyi parçalara ayırarak her bir parçayı özetlemek ve ardından bu özetleri birleştirerek nihai bir özet oluşturmak için kullanılır. Özetleme işlemi, Map-Reduce yöntemiyle yapılır ve özel olarak tanımlanmış prompt şablonları ile özetleme süreci yönlendirilir. Sonuçta elde edilen özet, Markdown formatında kullanıcıya sunulur."
      ],
      "metadata": {
        "id": "6us_YSEe3u9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu metin, BERT modelinin doğal dil işleme alanında nasıl devrim yarattığını ve önceki modellere göre sunduğu avantajları özetlemektedir.\n",
        "\n",
        "BERT'in Tanıtımı: BERT, Google tarafından geliştirilen ve hem soldan hem de sağdan bağlamı dikkate alarak derin dil temsilleri oluşturan bir modeldir.\n",
        "Ön Eğitim ve İnce Ayar: BERT, farklı NLP görevleri için minimal ince ayar gerektirir ve birçok benchmark'ta en iyi performansı göstermiştir.\n",
        "Mevcut Modellerin Eleştirisi: Tek yönlü modellerin sınırlamalarını aşmak için BERT, \"Masked Language Model\" (MLM) tekniğini kullanır.\n",
        "Ana Katkılar: BERT, iki yönlü ön eğitim ve karmaşık görev-specifik mimarilere olan ihtiyacı azaltmasıyla öne çıkar.\n",
        "Performans: BERT, çeşitli NLP görevlerinde önceki modellere göre üstün performans sergileyerek yeni standartlar belirlemiştir."
      ],
      "metadata": {
        "id": "QPJ8u2-u34Qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "FLJmnz9TVCRL"
      }
    }
  ]
}