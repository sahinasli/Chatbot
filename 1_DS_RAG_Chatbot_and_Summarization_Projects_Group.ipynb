{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f1245a8ab8048769fb69303051e6361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eb751a695594484a7ff91e021b02b29",
              "IPY_MODEL_90a25edea652435a830e68d5d11f9a1a",
              "IPY_MODEL_272a1274b7994a919cb1b26413774022"
            ],
            "layout": "IPY_MODEL_447e8ccc99074fa59228653842d5dce9"
          }
        },
        "2eb751a695594484a7ff91e021b02b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_965174e5f386481492a003b788d81a2d",
            "placeholder": "​",
            "style": "IPY_MODEL_1347301bf14b4fbca4d1bf815190930a",
            "value": "Map: 100%"
          }
        },
        "90a25edea652435a830e68d5d11f9a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f9134e695b418880bdb0f63c22e74c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_161e05c2e7414b7c93639d5f7ad3515e",
            "value": 3
          }
        },
        "272a1274b7994a919cb1b26413774022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2be29f90ae944c839458b0cea50963d9",
            "placeholder": "​",
            "style": "IPY_MODEL_596f55e9f18646d1b7a7203c479c4f20",
            "value": " 3/3 [00:00&lt;00:00, 52.90 examples/s]"
          }
        },
        "447e8ccc99074fa59228653842d5dce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965174e5f386481492a003b788d81a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1347301bf14b4fbca4d1bf815190930a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03f9134e695b418880bdb0f63c22e74c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "161e05c2e7414b7c93639d5f7ad3515e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2be29f90ae944c839458b0cea50963d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "596f55e9f18646d1b7a7203c479c4f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0412aef1bdba453bb74885140e854d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beea8ad78f6f44c2ad90cf6127ee6a34",
              "IPY_MODEL_dfbde95f3b0142ec8c887933f7492351",
              "IPY_MODEL_ed97ab7cb651470a86568ddb1c0f83ab"
            ],
            "layout": "IPY_MODEL_98f7d4db8c8949138c9c39a3c000071f"
          }
        },
        "beea8ad78f6f44c2ad90cf6127ee6a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9afd318a5d94c88b066a7c79bfb5a3e",
            "placeholder": "​",
            "style": "IPY_MODEL_acb7fd46fe02476292ed27ef5121afa8",
            "value": "Casting the dataset: 100%"
          }
        },
        "dfbde95f3b0142ec8c887933f7492351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f297966ce8405f8e93b8f11873b3ef",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d26eb827374ec1b386f5134187c71e",
            "value": 3
          }
        },
        "ed97ab7cb651470a86568ddb1c0f83ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_901acbc0b44440e4beeaba52fd13e270",
            "placeholder": "​",
            "style": "IPY_MODEL_c4e781668bc54a74a06dea9245f3793f",
            "value": " 3/3 [00:00&lt;00:00, 41.79 examples/s]"
          }
        },
        "98f7d4db8c8949138c9c39a3c000071f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9afd318a5d94c88b066a7c79bfb5a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb7fd46fe02476292ed27ef5121afa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1f297966ce8405f8e93b8f11873b3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d26eb827374ec1b386f5134187c71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "901acbc0b44440e4beeaba52fd13e270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e781668bc54a74a06dea9245f3793f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f7e6b19543448aa87153c7884a4d4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b991f35775e4e02bbf2fc5b9962d10f",
              "IPY_MODEL_995aa7ebee484a93a5a35bae75b29f80",
              "IPY_MODEL_1e35135881784f068f444c872cd8d9a0"
            ],
            "layout": "IPY_MODEL_4c2605fd4ca1439c8b275885924cd4af"
          }
        },
        "3b991f35775e4e02bbf2fc5b9962d10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc10f3717c044d7889d9489bd5de9108",
            "placeholder": "​",
            "style": "IPY_MODEL_b11dbe22a9284d388d03a48dc438a523",
            "value": "Evaluating: 100%"
          }
        },
        "995aa7ebee484a93a5a35bae75b29f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b8996f953ea4d16a68ce9345f6e4478",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e8cca6559114cf59226d8d5995312a9",
            "value": 12
          }
        },
        "1e35135881784f068f444c872cd8d9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54fc2f325ac34875851647354cfe9fb5",
            "placeholder": "​",
            "style": "IPY_MODEL_292ed12665d44f11ab01088dfa5f21b5",
            "value": " 12/12 [03:00&lt;00:00, 24.43s/it]"
          }
        },
        "4c2605fd4ca1439c8b275885924cd4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc10f3717c044d7889d9489bd5de9108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b11dbe22a9284d388d03a48dc438a523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b8996f953ea4d16a68ce9345f6e4478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8cca6559114cf59226d8d5995312a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54fc2f325ac34875851647354cfe9fb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "292ed12665d44f11ab01088dfa5f21b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "Jp0pcBxsVAMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WELCOME\n",
        "\n",
        "This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text.\n",
        "\n",
        "Through two distinct projects, you will explore these technologies and enhance your skills. Detailed descriptions of the projects are provided below."
      ],
      "metadata": {
        "id": "DxOaSxtJWV1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 1: Building a Chatbot with a PDF Document (RAG)\n",
        "\n",
        "In this project, you will develop a chatbot using a provided PDF document from web page. You will utilize the Langchain framework along with a large language model (LLM) such as GPT or Gemini. The chatbot will leverage the Retrieval Augmented Generation (RAG) technique to comprehend the document's content and respond to user queries effectively.\n",
        "\n",
        "### **Project Steps:**\n",
        "\n",
        "- **1.PDF Document Upload:** Upload the provided PDF document from web page (https://aclanthology.org/N19-1423.pdf) (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding).\n",
        "\n",
        "- **2.Chunking:** Divide the uploaded PDF document into smaller segments (chunks). This facilitates more efficient information processing by the LLM.\n",
        "\n",
        "- **3.ChromaDB Setup:**\n",
        "  - Save ChromaDB to your Google Drive.\n",
        "\n",
        "  - Retrieve ChromaDB from your Drive to begin using it in your project.\n",
        "\n",
        "  - ChromaDB serves as a vector database to store embedding vectors generated from your document.\n",
        "\n",
        "- **4.Embedding Vectors Creation:**\n",
        "  - Convert the chunked document into embedding vectors. You can use either GPT or Gemini embedding models for this purpose.\n",
        "\n",
        "  - If you choose the Gemini embedding model, set \"task_type\" to \"retrieval_document\" when converting the chunked document.\n",
        "\n",
        "- **5.Chatbot Development:**\n",
        "  - Utilize the **load_qa_chain** function from the Langchain library to build the chatbot.\n",
        "\n",
        "  - This function will interpret user queries, retrieve relevant information from **ChromaDB**, and generate responses accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "MaCz7nhxKI9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "_eoQWi-uN0dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-community"
      ],
      "metadata": {
        "id": "PCbI4MuNanVu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community"
      ],
      "metadata": {
        "id": "qOaahY-AancA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai\n"
      ],
      "metadata": {
        "id": "ZZLMILzfEga_",
        "outputId": "4b5e32df-7120-4fa5-c439-d531032c0bb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.10/dist-packages (1.0.10)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.7.2)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.33 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.2.37)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.28.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n",
            "Collecting protobuf (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (0.1.108)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.33->langchain-google-genai) (8.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.20.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.66.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.33->langchain-google-genai) (1.2.2)\n",
            "Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.28.0\n",
            "    Uninstalling protobuf-5.28.0:\n",
            "      Successfully uninstalled protobuf-5.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "grpcio-health-checking 1.66.1 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.4 which is incompatible.\n",
            "grpcio-tools 1.66.1 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.4 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-chroma"
      ],
      "metadata": {
        "id": "SFYZShShqYkz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pypdfium2"
      ],
      "metadata": {
        "id": "3wmw_2hbqYnu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "zRZkTVi1Our9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "PNsz4Jo4vwhU",
        "outputId": "d3b4dff0-4a2a-41d6-983c-ce20d1e619ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.112.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.4)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.66.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.64.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.8.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "metadata": {
        "id": "luXDfzEOi83Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "SqA0v6nzRFHf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "5lj8WYKeOnNQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai weaviate-client ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GykpNb6OpJ3",
        "outputId": "a9ea8d68-54f2-43b6-a8d8-c13fdb5b01ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.15)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.43.0)\n",
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.35 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.37)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.108)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: validators==0.33.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.33.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.3.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.66.1)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.66.1)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.66.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas) (2.21.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas) (0.7.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas) (0.2.15)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from ragas) (0.1.23)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (43.0.0)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client)\n",
            "  Using cached protobuf-5.28.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (71.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.23.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2024.5.15)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
            "Using cached protobuf-5.28.0-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.4\n",
            "    Uninstalling protobuf-4.25.4:\n",
            "      Successfully uninstalled protobuf-4.25.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.28.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "opentelemetry-proto 1.27.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access Google Drive"
      ],
      "metadata": {
        "id": "FuLllnCl2yfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vBdmv-Mu4k_",
        "outputId": "c7adf11d-727b-4ec2-a1ae-d4604542b21d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entering Your OpenAI or Google Gemini API Key."
      ],
      "metadata": {
        "id": "0uR9bJp_0MyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OpenAIkey')"
      ],
      "metadata": {
        "id": "2jwo1SQ2asnZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Document"
      ],
      "metadata": {
        "id": "OV9rG-0PN8p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pdf reader function\n",
        "from langchain_community.document_loaders import PyPDFium2Loader\n",
        "\n",
        "def read_doc(directory):\n",
        "    file_loader=PyPDFium2Loader(directory)\n",
        "    pdf_documents=file_loader.load() # PyPDFium2Loader reads page by page\n",
        "    return pdf_documents"
      ],
      "metadata": {
        "id": "5H6eQyYyauxP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf')\n",
        "len(pdf)\n",
        "\n",
        "# The document consists of 16 pages"
      ],
      "metadata": {
        "id": "n_kXJZ5Taupv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd3fc1c-273d-428b-ac43-ae742331b221"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc0CiWk3zU3c",
        "outputId": "fa3c416d-a67e-4785-b7b5-e713f514f87a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Splitter"
      ],
      "metadata": {
        "id": "WLQ1j_JrOF57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "\n",
        "def chunk_data(docs, chunk_size=1000, chunk_overlap=200):\n",
        "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                 chunk_overlap=chunk_overlap)\n",
        "    pdf=text_splitter.split_documents(docs)\n",
        "    return pdf\n",
        "\n",
        "# This code splits documents into chunks using the RecursiveCharacterTextSplitter class from the langchain library.\n",
        "\n",
        "# A function named chunk_data is defined, which takes a document or a collection of documents (docs) as input. It also takes two parameters:\n",
        "# chunk_size and chunk_overlap. chunk_size specifies the maximum number of characters in each chunk, while chunk_overlap determines the amount of\n",
        "# overlap between consecutive chunks.\n",
        "\n",
        "# The function divides the documents into chunks based on these parameters using the RecursiveCharacterTextSplitter class. Consequently, each chunk\n",
        "# contains chunk_size characters, with an overlap of chunk_overlap characters between consecutive chunks.\n",
        "\n",
        "# As a result, the documents are segmented into chunks of specified sizes, and these chunks are returned.\n",
        "\n",
        "# The chunk_overlap parameter is used to specify the sharing of characters between consecutive chunks. In other words, it ensures that the characters at\n",
        "# the end of one chunk reappear at the beginning of the next chunk. This prevents the loss of information when the text is segmented or divided and\n",
        "# helps preserve a certain context. Especially, overlap can be used to maintain important contextual relationships within a specific text and sustain\n",
        "# meaning across chunks."
      ],
      "metadata": {
        "id": "HHQlclU9awwa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc=chunk_data(docs=pdf)\n",
        "len(pdf_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MVmbISDzps_",
        "outputId": "9b3b947d-4db0-41e3-bb5f-90acf8769935"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pdf_doc[25:27]"
      ],
      "metadata": {
        "id": "NaQV6XRwawpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcdce66-56bc-4c45-827a-64470b6552d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 4}, page_content='answering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 4}, page_content='included in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Creating A Embedding Model\n",
        "### 2. Convert the Each Chunk of The Split Document to Embedding Vectors\n",
        "### 3. Storing of The Embedding Vectors to Vectorstore\n",
        "### 4. Save the Vectorstore to Your Drive"
      ],
      "metadata": {
        "id": "4ENim_5MOT9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
        "                                          dimensions = 3072)# dimensions=256, 1024, 3072\n",
        "\n",
        "\n",
        "\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "96nLFF1ja0k_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee1d286-4925-479f-a49c-8223c2bb4383"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<openai.resources.embeddings.Embeddings object at 0x78bfdaaeb9d0> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x78bfdab78b80> model='text-embedding-3-large' dimensions=3072 deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Vectorstore(index) From Your Drive"
      ],
      "metadata": {
        "id": "y2tMqUthPchD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "#index=Chroma.from_documents(documents=pdf_doc,\n",
        "#                            embedding=embeddings,\n",
        "#                            persist_directory=\"/content/drive/MyDrive/vectorstore\") # persist_directory, saves in the directory\n",
        "\n",
        "#retriever=index.as_retriever()"
      ],
      "metadata": {
        "id": "5pjKXmO6a3En"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index=Chroma(persist_directory=\"/content/drive/MyDrive/vectorstore\",\n",
        "                    embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "8VTutoj7OV1l"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_retriever=loaded_index.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "Rl-YMVFnN9au"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrival the First 5 Chunks That Are Most Similar to The User Query from The Document"
      ],
      "metadata": {
        "id": "pKA0PgNJQOmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_retriever = loaded_index.as_retriever(search_kwargs = {'k': 5})"
      ],
      "metadata": {
        "id": "KRH-FWEua5Fn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = load_retriever.invoke('What is BERT?')"
      ],
      "metadata": {
        "id": "80TWdFk6a4-3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "id": "25SNQ8uTR0KB",
        "outputId": "d71b9ff8-5bd3-45e0-dd51-05319df3f0de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='example, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='example, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked'),\n",
              " Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating an Answer Based on The Similar Chunks"
      ],
      "metadata": {
        "id": "-G8R4V7BROkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "----------------\n",
        "{context}\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables =['question','context'],\n",
        "    template = template\n",
        ")"
      ],
      "metadata": {
        "id": "XNDU0jcma7HB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline For RAG (If you want, you can use the gemini-1.5-pro model)"
      ],
      "metadata": {
        "id": "sy4fmzsWLayT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\""
      ],
      "metadata": {
        "id": "WDdnBrV-Suxh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "               temperature=0,\n",
        "               top_p=1)\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "output= chain.invoke({\"question\":our_query, \"context\":retrieved_docs}) # first four most similar texts are returned\n",
        "output"
      ],
      "metadata": {
        "id": "aUnTIxSoa6_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9f14c59c-ef2c-4bac-beee-fb9ff7dba078"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation model that improves fine-tuning approaches for natural language processing tasks. It is designed to address the limitations of unidirectional models by using a \"masked language model\" (MLM) pre-training objective, allowing it to incorporate context from both directions. BERT has achieved state-of-the-art results on various natural language processing tasks, including question answering and language inference, without requiring significant modifications to task-specific architectures.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(output)"
      ],
      "metadata": {
        "id": "5UrWtZ776nil",
        "outputId": "29072753-94d3-4d8a-ee6d-5549a2ac65c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation model that improves fine-tuning approaches for natural language processing tasks. It is designed to address the limitations of unidirectional models by using a \"masked language model\" (MLM) pre-training objective, allowing it to incorporate context from both directions. BERT has achieved state-of-the-art results on various natural language processing tasks, including question answering and language inference, without requiring significant modifications to task-specific architectures."
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Retrieval**"
      ],
      "metadata": {
        "id": "8vbuPptARJrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_query(query,k=5):\n",
        "    load_retriever=loaded_index.as_retriever(search_kwargs={\"k\": k}) #loaded_index\n",
        "    return load_retriever.invoke(query)"
      ],
      "metadata": {
        "id": "VYc2ddhORJGt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is BERT?\"\n",
        "\n",
        "doc_search=retrieve_query(our_query, k=5) # first two most similar texts are returned\n",
        "doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEyWH1V7Rgv_",
        "outputId": "ba726958-492c-46f7-df7b-5c6035a384a9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='to create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='example, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked'),\n",
              " Document(metadata={'page': 0, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='example, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked'),\n",
              " Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf'}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answers(query, k = 5):\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from IPython.display import Markdown\n",
        "\n",
        "    doc_search=retrieve_query(query, k = k) # most similar texts are returned\n",
        "\n",
        "\n",
        "    template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    ----------------\n",
        "    {context}\"\"\"\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "    input_variables =['question','context'],\n",
        "    template = template)\n",
        "\n",
        "\n",
        "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "                  temperature=0,\n",
        "                  top_p=1)\n",
        "\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    output= chain.invoke({\"question\":query, \"context\":doc_search}) # first four most similar texts are returned\n",
        "    return Markdown(output)"
      ],
      "metadata": {
        "id": "Na1X4-QXOo-V"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What are the key architectural components of BERT?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "KUqWSAW-QGin",
        "outputId": "2772425f-8830-4db6-cbab-cc09307d83b6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The key architectural components of BERT include:\n\n1. **Multi-layer Bidirectional Transformer Encoder**: BERT is based on a multi-layer bidirectional Transformer architecture, which allows it to consider context from both the left and right sides of a token.\n\n2. **Unified Architecture**: There is minimal difference between the pre-trained architecture and the final downstream architecture, allowing for a consistent approach across different tasks.\n\n3. **Model Sizes**: BERT primarily comes in two sizes:\n   - **BERTBASE**: 12 layers (Transformer blocks), hidden size of 768, and 12 self-attention heads, totaling 110 million parameters.\n   - **BERTLARGE**: 24 layers, hidden size of 1024, with a larger number of parameters.\n\n4. **Input Representation**: BERT uses special tokens such as [CLS] for classification tasks and [SEP] to separate different segments of input.\n\n5. **Pre-training and Fine-tuning**: The same architecture is used for both pre-training and fine-tuning, with all parameters being fine-tuned during the latter phase.\n\nThese components contribute to BERT's ability to effectively understand and generate language representations for various natural language processing tasks."
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What are the main contributions of the BERT model?\"\n",
        "answer = get_answers(our_query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "gbTmfKHPRvBO",
        "outputId": "1439b28e-9267-468c-c03b-6e87cd3e9331"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The main contributions of the BERT model include:\n\n1. **Bidirectional Pre-training**: BERT demonstrates the importance of bidirectional pre-training for language representations. Unlike previous models that used unidirectional language models, BERT employs masked language models to enable deep bidirectional representations.\n\n2. **State-of-the-Art Performance**: BERT achieves new state-of-the-art results on eleven natural language processing tasks, including significant improvements in scores for benchmarks like GLUE, MultiNLI, and SQuAD.\n\n3. **Flexibility for Various Tasks**: The model can be fine-tuned with just one additional output layer, allowing it to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n\n4. **Joint Conditioning on Context**: BERT is designed to pre-train representations by jointly conditioning on both left and right context in all layers, which enhances its understanding of language. \n\nThese contributions highlight BERT's effectiveness and versatility in natural language processing."
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    template=\"Based on the following context:\\n{context}\\n\\nQ: {question}\\nA:\"\n",
        ")\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    top_p=1\n",
        ")\n",
        "\n",
        "# Create the LLM chain\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "# Define the questions, ground truths, and placeholders for answers and contexts\n",
        "questions = [\n",
        "    \"What strategy does BERT use in the masked language model (MLM) task, and why?\",\n",
        "    \"What are the advantages of using a feature-based approach with BERT compared to the fine-tuning approach?\",\n",
        "    \"What is the major contribution of recent improvements in transfer learning with language models?\",\n",
        "]\n",
        "ground_truths = [\n",
        "    [\"BERT replaces 15% of input tokens, using [MASK] 80% of the time, a random token 10%, and leaves the token unchanged 10% of the time. This strategy helps the model learn bidirectional representations while reducing the difference between pre-training and fine-tuning.\"],\n",
        "    [\"The feature-based approach has advantages such as allowing for task-specific model architectures when the Transformer encoder alone isn't suitable, and offering computational benefits by pre-computing expensive representations once. This makes it possible to run multiple experiments with simpler models on top of these pre-computed features.\"],\n",
        "    [\"The major contribution is extending the benefits of rich, unsupervised pre-training to deep bidirectional architectures. This advancement allows pre-trained models to effectively handle a wide range of NLP tasks, even in low-resource settings, by leveraging the strengths of deep bidirectional models.\"]\n",
        "]\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "# Inference\n",
        "for query in questions:\n",
        "    # Get relevant documents for the query\n",
        "    relevant_docs = load_retriever.get_relevant_documents(query)\n",
        "    # Join the documents' content to form the context\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Create input dictionary for the chain\n",
        "    inputs = {\"question\": query, \"context\": context}\n",
        "\n",
        "    # Run the chain and get the output\n",
        "    output = chain.invoke(inputs)\n",
        "\n",
        "    # Append the output and context\n",
        "    answers.append(output)\n",
        "    contexts.append(context)\n",
        "\n",
        "# Prepare the data dictionary\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truths\": ground_truths\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a Dataset\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg3rA_DANrVE",
        "outputId": "edea248c-59ab-4419-b62b-a7642d0bd749"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-bd7da19d9fb4>:39: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
            "  relevant_docs = load_retriever.get_relevant_documents(query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Features, Sequence, Value\n",
        "\n",
        "# Convert `contexts` to a list of strings if it isn't already\n",
        "def convert_contexts_to_list_of_strings(dataset):\n",
        "    # Ensure the contexts field is a list of strings\n",
        "    def to_list_of_strings(x):\n",
        "        if isinstance(x, str):\n",
        "            return [x]  # Convert single string to a list containing one string\n",
        "        return x  # If already a list, return it unchanged\n",
        "\n",
        "    # Apply transformation\n",
        "    new_data = dataset.map(lambda x: {\"contexts\": to_list_of_strings(x[\"contexts\"])}, batched=False)\n",
        "\n",
        "    # Define the correct features\n",
        "    features = Features({\n",
        "        \"question\": Value(dtype=\"string\"),\n",
        "        \"answer\": Value(dtype=\"string\"),\n",
        "        \"contexts\": Sequence(Value(dtype=\"string\")),\n",
        "        \"ground_truths\": Sequence(Value(dtype=\"string\")),\n",
        "    })\n",
        "\n",
        "    # Re-create the dataset with correct features\n",
        "    dataset = new_data.cast(features)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Apply the conversion function\n",
        "dataset = convert_contexts_to_list_of_strings(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "1f1245a8ab8048769fb69303051e6361",
            "2eb751a695594484a7ff91e021b02b29",
            "90a25edea652435a830e68d5d11f9a1a",
            "272a1274b7994a919cb1b26413774022",
            "447e8ccc99074fa59228653842d5dce9",
            "965174e5f386481492a003b788d81a2d",
            "1347301bf14b4fbca4d1bf815190930a",
            "03f9134e695b418880bdb0f63c22e74c",
            "161e05c2e7414b7c93639d5f7ad3515e",
            "2be29f90ae944c839458b0cea50963d9",
            "596f55e9f18646d1b7a7203c479c4f20",
            "0412aef1bdba453bb74885140e854d5b",
            "beea8ad78f6f44c2ad90cf6127ee6a34",
            "dfbde95f3b0142ec8c887933f7492351",
            "ed97ab7cb651470a86568ddb1c0f83ab",
            "98f7d4db8c8949138c9c39a3c000071f",
            "f9afd318a5d94c88b066a7c79bfb5a3e",
            "acb7fd46fe02476292ed27ef5121afa8",
            "b1f297966ce8405f8e93b8f11873b3ef",
            "73d26eb827374ec1b386f5134187c71e",
            "901acbc0b44440e4beeaba52fd13e270",
            "c4e781668bc54a74a06dea9245f3793f"
          ]
        },
        "id": "k9tW1q6LNwid",
        "outputId": "7f0933b9-ee73-4d65-c9a5-7a91240e10f0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f1245a8ab8048769fb69303051e6361"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0412aef1bdba453bb74885140e854d5b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "# Evaluate the dataset using the specified metrics\n",
        "result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Convert the result to a pandas DataFrame\n",
        "df = result.to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "1f7e6b19543448aa87153c7884a4d4ba",
            "3b991f35775e4e02bbf2fc5b9962d10f",
            "995aa7ebee484a93a5a35bae75b29f80",
            "1e35135881784f068f444c872cd8d9a0",
            "4c2605fd4ca1439c8b275885924cd4af",
            "bc10f3717c044d7889d9489bd5de9108",
            "b11dbe22a9284d388d03a48dc438a523",
            "9b8996f953ea4d16a68ce9345f6e4478",
            "3e8cca6559114cf59226d8d5995312a9",
            "54fc2f325ac34875851647354cfe9fb5",
            "292ed12665d44f11ab01088dfa5f21b5"
          ]
        },
        "id": "7I9peCrlXtpQ",
        "outputId": "a7d1ca51-432a-4a6a-8b38-b590d9e989b1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ragas.validation:passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f7e6b19543448aa87153c7884a4d4ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ragas.executor:Exception raised in Job[10]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[6]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[2]: TimeoutError()\n",
            "ERROR:ragas.executor:Exception raised in Job[3]: TimeoutError()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "EUjB58TiX0-L",
        "outputId": "0845ec60-9bcb-45f7-a162-c8c5c5b8abbe"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  What strategy does BERT use in the masked lang...   \n",
              "1  What are the advantages of using a feature-bas...   \n",
              "2  What is the major contribution of recent impro...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  BERT uses a masking strategy in the masked lan...   \n",
              "1  The advantages of using a feature-based approa...   \n",
              "2  The major contribution of recent improvements ...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [right-to-left language models to pre-train BE...   \n",
              "1  [all parameters are jointly fine-tuned on a do...   \n",
              "2  [the classification layer.\\r\\nResults are pres...   \n",
              "\n",
              "                                       ground_truths  \\\n",
              "0  [BERT replaces 15% of input tokens, using [MAS...   \n",
              "1  [The feature-based approach has advantages suc...   \n",
              "2  [The major contribution is extending the benef...   \n",
              "\n",
              "                                        ground_truth  context_precision  \\\n",
              "0  BERT replaces 15% of input tokens, using [MASK...                1.0   \n",
              "1  The feature-based approach has advantages such...                1.0   \n",
              "2  The major contribution is extending the benefi...                1.0   \n",
              "\n",
              "   context_recall  faithfulness  answer_relevancy  \n",
              "0             1.0           NaN               NaN  \n",
              "1             1.0           NaN               1.0  \n",
              "2             1.0           NaN               1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c194be2-b5f5-4b68-b6da-616d10267395\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truths</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What strategy does BERT use in the masked lang...</td>\n",
              "      <td>BERT uses a masking strategy in the masked lan...</td>\n",
              "      <td>[right-to-left language models to pre-train BE...</td>\n",
              "      <td>[BERT replaces 15% of input tokens, using [MAS...</td>\n",
              "      <td>BERT replaces 15% of input tokens, using [MASK...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are the advantages of using a feature-bas...</td>\n",
              "      <td>The advantages of using a feature-based approa...</td>\n",
              "      <td>[all parameters are jointly fine-tuned on a do...</td>\n",
              "      <td>[The feature-based approach has advantages suc...</td>\n",
              "      <td>The feature-based approach has advantages such...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the major contribution of recent impro...</td>\n",
              "      <td>The major contribution of recent improvements ...</td>\n",
              "      <td>[the classification layer.\\r\\nResults are pres...</td>\n",
              "      <td>[The major contribution is extending the benef...</td>\n",
              "      <td>The major contribution is extending the benefi...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c194be2-b5f5-4b68-b6da-616d10267395')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c194be2-b5f5-4b68-b6da-616d10267395 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c194be2-b5f5-4b68-b6da-616d10267395');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3f5d8a46-317b-40da-9c7c-e1d6acc2bde4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f5d8a46-317b-40da-9c7c-e1d6acc2bde4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3f5d8a46-317b-40da-9c7c-e1d6acc2bde4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4a7ba171-f9bb-4a87-8ced-4f78d43687c4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4a7ba171-f9bb-4a87-8ced-4f78d43687c4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"What strategy does BERT use in the masked language model (MLM) task, and why?\",\n          \"What are the advantages of using a feature-based approach with BERT compared to the fine-tuning approach?\",\n          \"What is the major contribution of recent improvements in transfer learning with language models?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BERT uses a masking strategy in the masked language model (MLM) task that involves replacing some percentage of the input tokens with a [MASK] symbol, while also incorporating two other strategies: keeping the target token as is (SAME) and replacing the target token with a random token (RND). Specifically, BERT employs these strategies with probabilities of 80% for MASK, 10% for SAME, and 10% for RND.\\n\\nThe rationale behind this approach is to create a deep bidirectional representation of the input text. By masking tokens, BERT allows the model to learn context from both the left and right sides of the masked token, which is crucial for understanding the meaning of words in context. This bidirectional conditioning enhances the model's ability to predict the masked tokens based on the surrounding context, making it more powerful than traditional left-to-right or right-to-left models. This method also helps to avoid the trivial prediction problem that arises when a model can \\\"see\\\" the target word during training.\",\n          \"The advantages of using a feature-based approach with BERT compared to the fine-tuning approach include:\\n\\n1. **Task-Specific Model Architecture**: Not all tasks can be easily represented by a Transformer encoder architecture. The feature-based approach allows for the integration of task-specific model architectures that may be better suited for certain tasks.\\n\\n2. **Computational Efficiency**: The feature-based approach enables the pre-computation of expensive representations of the training data. This allows researchers to run multiple experiments with cheaper models on top of these pre-computed representations, saving computational resources and time.\\n\\n3. **Flexibility in Experimentation**: By extracting fixed features from the pre-trained model, researchers can experiment with various downstream tasks without the need to retrain the entire model. This flexibility can lead to quicker iterations and testing of different hypotheses.\\n\\n4. **Potential for Better Performance with Limited Data**: The feature-based approach may benefit from larger, more expressive pre-trained representations, especially when the downstream task data is limited. This can lead to improved performance even with a small amount of task-specific data.\\n\\n5. **Simplicity in Implementation**: The feature-based approach can be conceptually simpler, as it involves using the pre-trained model's outputs as features for a separate model, rather than modifying and fine-tuning the entire architecture.\\n\\nOverall, while fine-tuning can lead to state-of-the-art results, the feature-based approach offers distinct advantages in terms of flexibility, efficiency, and the ability to leverage pre-trained representations effectively.\",\n          \"The major contribution of recent improvements in transfer learning with language models is the demonstration that rich, unsupervised pre-training is an integral part of many language understanding systems. This approach allows even low-resource tasks to benefit from deep unidirectional architectures, and the findings have been further generalized to deep bidirectional architectures, enabling the same pre-trained model to successfully tackle a broad set of NLP tasks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truths\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BERT replaces 15% of input tokens, using [MASK] 80% of the time, a random token 10%, and leaves the token unchanged 10% of the time. This strategy helps the model learn bidirectional representations while reducing the difference between pre-training and fine-tuning.\",\n          \"The feature-based approach has advantages such as allowing for task-specific model architectures when the Transformer encoder alone isn't suitable, and offering computational benefits by pre-computing expensive representations once. This makes it possible to run multiple experiments with simpler models on top of these pre-computed features.\",\n          \"The major contribution is extending the benefits of rich, unsupervised pre-training to deep bidirectional architectures. This advancement allows pre-trained models to effectively handle a wide range of NLP tasks, even in low-resource settings, by leveraging the strengths of deep bidirectional models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.9999999999,\n        \"max\": 0.9999999999,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.108895957933346e-16,\n        \"min\": 0.9999999999999994,\n        \"max\": 1.0000000000000004,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 2: Generating PDF Document Summaries\n",
        "\n",
        "In this project, you will explore various methods for creating summaries from the provided PDF document. You will experiment with different chaining functions offered by the Langchain library to achieve this.\n",
        "\n",
        "### **Project Steps:**\n",
        "- **1.PDF Document Upload and Chunking:** As in the first project, upload the PDF document and divide it into smaller chunks. Consider splitting it by half-page or page.\n",
        "\n",
        "- **2.Summarization Techniques:**\n",
        "\n",
        "  - **Summary of the First 5 Pages (Stuff Chain):** Utilize the load_summarize_chain function with the parameter chain_type=\"stuff\" to generate a concise summary of the first 5 pages of the PDF document.\n",
        "\n",
        "  - **Short Summary of the Entire Document (Map Reduce Chain):** Employ chain_type=\"map_reduce\" and refine parameters to create a brief summary of the entire document. This method generates individual summaries for each chunk and then combines them into a final summary.\n",
        "\n",
        "  - **Detailed Summary with Bullet Points (Map Reduce Chain):** Use chain_type=\"map_reduce\" to generate a detailed summary with at least 1000 tokens. Provide the LLM with the prompt \"Summarize with 1000 tokens\" and set the max_token parameter to a value greater than 1000. Add a title to the summary and present key points using bullet points.\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "- Models like GPT-4 and Gemini Pro models might excel in generating summaries based on token count. Consider prioritizing these models.\n",
        "\n",
        "- For comprehensive information on Langchain and LLMs, refer to their respective documentation.\n",
        "Best of luck!"
      ],
      "metadata": {
        "id": "H9GmKlL2NRff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "WhjLe0IqRnl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ZXdV8CcqbFrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading PDF Document"
      ],
      "metadata": {
        "id": "yqImlx_IRqQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=read_doc('/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf')"
      ],
      "metadata": {
        "id": "CCkT3msfbH_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdd007a-97b5-41b0-82f6-5ccc64dfd9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[0] #first page"
      ],
      "metadata": {
        "id": "5a_FpBOcbHzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1f706c-06a4-418e-ef2d-b7026bdf4393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing the First 5 Pages of The Document With Chain_Type of The 'stuff'"
      ],
      "metadata": {
        "id": "LuyT0IoWR4n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import load_summarize_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "O3yAnW3PbKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf[:5] # first five page"
      ],
      "metadata": {
        "id": "8wopgGPibKA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce613b89-73f7-4e33-bb3b-ae7e0adcb59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 1}, page_content='4172\\r\\nword based only on its context. Unlike left-to\\x02right language model pre-training, the MLM ob\\x02jective enables the representation to fuse the left\\r\\nand the right context, which allows us to pre\\x02train a deep bidirectional Transformer. In addi\\x02tion to the masked language model, we also use\\r\\na “next sentence prediction” task that jointly pre\\x02trains text-pair representations. The contributions\\r\\nof our paper are as follows:\\r\\n• We demonstrate the importance of bidirectional\\r\\npre-training for language representations. Un\\x02like Radford et al. (2018), which uses unidirec\\x02tional language models for pre-training, BERT\\r\\nuses masked language models to enable pre\\x02trained deep bidirectional representations. This\\r\\nis also in contrast to Peters et al. (2018a), which\\r\\nuses a shallow concatenation of independently\\r\\ntrained left-to-right and right-to-left LMs.\\r\\n• We show that pre-trained representations reduce\\r\\nthe need for many heavily-engineered task\\x02specific architectures. BERT is the first fine\\x02tuning based representation model that achieves\\r\\nstate-of-the-art performance on a large suite\\r\\nof sentence-level and token-level tasks, outper\\x02forming many task-specific architectures.\\r\\n• BERT advances the state of the art for eleven\\r\\nNLP tasks. The code and pre-trained mod\\x02els are available at https://github.com/\\r\\ngoogle-research/bert.\\r\\n2 Related Work\\r\\nThere is a long history of pre-training general lan\\x02guage representations, and we briefly review the\\r\\nmost widely-used approaches in this section.\\r\\n2.1 Unsupervised Feature-based Approaches\\r\\nLearning widely applicable representations of\\r\\nwords has been an active area of research for\\r\\ndecades, including non-neural (Brown et al., 1992;\\r\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\r\\nneural (Mikolov et al., 2013; Pennington et al.,\\r\\n2014) methods. Pre-trained word embeddings\\r\\nare an integral part of modern NLP systems, of\\x02fering significant improvements over embeddings\\r\\nlearned from scratch (Turian et al., 2010). To pre\\x02train word embedding vectors, left-to-right lan\\x02guage modeling objectives have been used (Mnih\\r\\nand Hinton, 2009), as well as objectives to dis\\x02criminate correct from incorrect words in left and\\r\\nright context (Mikolov et al., 2013).\\r\\nThese approaches have been generalized to\\r\\ncoarser granularities, such as sentence embed\\x02dings (Kiros et al., 2015; Logeswaran and Lee,\\r\\n2018) or paragraph embeddings (Le and Mikolov,\\r\\n2014). To train sentence representations, prior\\r\\nwork has used objectives to rank candidate next\\r\\nsentences (Jernite et al., 2017; Logeswaran and\\r\\nLee, 2018), left-to-right generation of next sen\\x02tence words given a representation of the previous\\r\\nsentence (Kiros et al., 2015), or denoising auto\\x02encoder derived objectives (Hill et al., 2016).\\r\\nELMo and its predecessor (Peters et al., 2017,\\r\\n2018a) generalize traditional word embedding re\\x02search along a different dimension. They extract\\r\\ncontext-sensitive features from a left-to-right and a\\r\\nright-to-left language model. The contextual rep\\x02resentation of each token is the concatenation of\\r\\nthe left-to-right and right-to-left representations.\\r\\nWhen integrating contextual word embeddings\\r\\nwith existing task-specific architectures, ELMo\\r\\nadvances the state of the art for several major NLP\\r\\nbenchmarks (Peters et al., 2018a) including ques\\x02tion answering (Rajpurkar et al., 2016), sentiment\\r\\nanalysis (Socher et al., 2013), and named entity\\r\\nrecognition (Tjong Kim Sang and De Meulder,\\r\\n2003). Melamud et al. (2016) proposed learning\\r\\ncontextual representations through a task to pre\\x02dict a single word from both left and right context\\r\\nusing LSTMs. Similar to ELMo, their model is\\r\\nfeature-based and not deeply bidirectional. Fedus\\r\\net al. (2018) shows that the cloze task can be used\\r\\nto improve the robustness of text generation mod\\x02els.\\r\\n2.2 Unsupervised Fine-tuning Approaches\\r\\nAs with the feature-based approaches, the first\\r\\nworks in this direction only pre-trained word em\\x02bedding parameters from unlabeled text (Col\\x02lobert and Weston, 2008).\\r\\nMore recently, sentence or document encoders\\r\\nwhich produce contextual token representations\\r\\nhave been pre-trained from unlabeled text and\\r\\nfine-tuned for a supervised downstream task (Dai\\r\\nand Le, 2015; Howard and Ruder, 2018; Radford\\r\\net al., 2018). The advantage of these approaches\\r\\nis that few parameters need to be learned from\\r\\nscratch. At least partly due to this advantage,\\r\\nOpenAI GPT (Radford et al., 2018) achieved pre\\x02viously state-of-the-art results on many sentence\\x02level tasks from the GLUE benchmark (Wang\\r\\net al., 2018a). Left-to-right language model-\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 2}, page_content='4173\\r\\nBERT BERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nQuestion Paragraph\\r\\nStart/End Span\\r\\nBERT\\r\\nE[CLS] E1\\r\\n E[SEP] ... EN\\r\\nE1’ ... EM’\\r\\nC T1 T[SEP] ... TN\\r\\nT1’ ... TM’\\r\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\r\\nMasked Sentence A Masked Sentence B\\r\\nPre-training Fine-Tuning\\r\\nNSP Mask LM Mask LM\\r\\nUnlabeled Sentence A and B Pair \\r\\nSQuAD\\r\\nQuestion Answer Pair\\r\\nMNLI NER\\r\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\r\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\r\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\r\\ning and auto-encoder objectives have been used\\r\\nfor pre-training such models (Howard and Ruder,\\r\\n2018; Radford et al., 2018; Dai and Le, 2015).\\r\\n2.3 Transfer Learning from Supervised Data\\r\\nThere has also been work showing effective trans\\x02fer from supervised tasks with large datasets, such\\r\\nas natural language inference (Conneau et al.,\\r\\n2017) and machine translation (McCann et al.,\\r\\n2017). Computer vision research has also demon\\x02strated the importance of transfer learning from\\r\\nlarge pre-trained models, where an effective recipe\\r\\nis to fine-tune models pre-trained with Ima\\x02geNet (Deng et al., 2009; Yosinski et al., 2014).\\r\\n3 BERT\\r\\nWe introduce BERT and its detailed implementa\\x02tion in this section. There are two steps in our\\r\\nframework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\r\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\r\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\r\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\r\\nquestion-answering example in Figure 1 will serve\\r\\nas a running example for this section.\\r\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\r\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\r\\nthe tensor2tensor library.1 Because the use\\r\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\r\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\r\\nVaswani et al. (2017) as well as excellent guides\\r\\nsuch as “The Annotated Transformer.”2\\r\\nIn this work, we denote the number of layers\\r\\n(i.e., Transformer blocks) as L, the hidden size as\\r\\nH, and the number of self-attention heads as A.\\r\\n3\\r\\nWe primarily report results on two model sizes:\\r\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\r\\nA=16, Total Parameters=340M).\\r\\nBERTBASE was chosen to have the same model\\r\\nsize as OpenAI GPT for comparison purposes.\\r\\nCritically, however, the BERT Transformer uses\\r\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\r\\ntoken can only attend to context to its left.4\\r\\n1\\r\\nhttps://github.com/tensorflow/tensor2tensor\\r\\n2\\r\\nhttp://nlp.seas.harvard.edu/2018/04/03/attention.html\\r\\n3\\r\\nIn all cases we set the feed-forward/filter size to be 4H,\\r\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\r\\n4We note that in the literature the bidirectional Trans-\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 3}, page_content='4174\\r\\nInput/Output Representations To make BERT\\r\\nhandle a variety of down-stream tasks, our input\\r\\nrepresentation is able to unambiguously represent\\r\\nboth a single sentence and a pair of sentences\\r\\n(e.g., h Question, Answeri) in one token sequence.\\r\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\r\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\r\\nWe use WordPiece embeddings (Wu et al.,\\r\\n2016) with a 30,000 token vocabulary. The first\\r\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\r\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\r\\ntasks. Sentence pairs are packed together into a\\r\\nsingle sequence. We differentiate the sentences in\\r\\ntwo ways. First, we separate them with a special\\r\\ntoken ([SEP]). Second, we add a learned embed\\x02ding to every token indicating whether it belongs\\r\\nto sentence A or sentence B. As shown in Figure 1,\\r\\nwe denote input embedding as E, the final hidden\\r\\nvector of the special [CLS] token as C ∈ R\\r\\nH,\\r\\nand the final hidden vector for the i\\r\\nth input token\\r\\nas Ti ∈ R\\r\\nH.\\r\\nFor a given token, its input representation is\\r\\nconstructed by summing the corresponding token,\\r\\nsegment, and position embeddings. A visualiza\\x02tion of this construction can be seen in Figure 2.\\r\\n3.1 Pre-training BERT\\r\\nUnlike Peters et al. (2018a) and Radford et al.\\r\\n(2018), we do not use traditional left-to-right or\\r\\nright-to-left language models to pre-train BERT.\\r\\nInstead, we pre-train BERT using two unsuper\\x02vised tasks, described in this section. This step\\r\\nis presented in the left part of Figure 1.\\r\\nTask #1: Masked LM Intuitively, it is reason\\x02able to believe that a deep bidirectional model is\\r\\nstrictly more powerful than either a left-to-right\\r\\nmodel or the shallow concatenation of a left-to\\x02right and a right-to-left model. Unfortunately,\\r\\nstandard conditional language models can only be\\r\\ntrained left-to-right or right-to-left, since bidirec\\x02tional conditioning would allow each word to in\\x02directly “see itself”, and the model could trivially\\r\\npredict the target word in a multi-layered context.\\r\\nformer is often referred to as a “Transformer encoder” while\\r\\nthe left-context-only version is referred to as a “Transformer\\r\\ndecoder” since it can be used for text generation.\\r\\nIn order to train a deep bidirectional representa\\x02tion, we simply mask some percentage of the input\\r\\ntokens at random, and then predict those masked\\r\\ntokens. We refer to this procedure as a “masked\\r\\nLM” (MLM), although it is often referred to as a\\r\\nCloze task in the literature (Taylor, 1953). In this\\r\\ncase, the final hidden vectors corresponding to the\\r\\nmask tokens are fed into an output softmax over\\r\\nthe vocabulary, as in a standard LM. In all of our\\r\\nexperiments, we mask 15% of all WordPiece to\\x02kens in each sequence at random. In contrast to\\r\\ndenoising auto-encoders (Vincent et al., 2008), we\\r\\nonly predict the masked words rather than recon\\x02structing the entire input.\\r\\nAlthough this allows us to obtain a bidirec\\x02tional pre-trained model, a downside is that we\\r\\nare creating a mismatch between pre-training and\\r\\nfine-tuning, since the [MASK] token does not ap\\x02pear during fine-tuning. To mitigate this, we do\\r\\nnot always replace “masked” words with the ac\\x02tual [MASK] token. The training data generator\\r\\nchooses 15% of the token positions at random for\\r\\nprediction. If the i-th token is chosen, we replace\\r\\nthe i-th token with (1) the [MASK] token 80% of\\r\\nthe time (2) a random token 10% of the time (3)\\r\\nthe unchanged i-th token 10% of the time. Then,\\r\\nTi will be used to predict the original token with\\r\\ncross entropy loss. We compare variations of this\\r\\nprocedure in Appendix C.2.\\r\\nTask #2: Next Sentence Prediction (NSP)\\r\\nMany important downstream tasks such as Ques\\x02tion Answering (QA) and Natural Language Infer\\x02ence (NLI) are based on understanding the rela\\x02tionship between two sentences, which is not di\\x02rectly captured by language modeling. In order\\r\\nto train a model that understands sentence rela\\x02tionships, we pre-train for a binarized next sen\\x02tence prediction task that can be trivially gener\\x02ated from any monolingual corpus. Specifically,\\r\\nwhen choosing the sentences A and B for each pre\\x02training example, 50% of the time B is the actual\\r\\nnext sentence that follows A (labeled as IsNext),\\r\\nand 50% of the time it is a random sentence from\\r\\nthe corpus (labeled as NotNext). As we show\\r\\nin Figure 1, C is used for next sentence predic\\x02tion (NSP).5 Despite its simplicity, we demon\\x02strate in Section 5.1 that pre-training towards this\\r\\ntask is very beneficial to both QA and NLI. 6\\r\\n5The final model achieves 97%-98% accuracy on NSP.\\r\\n6The vector C is not a meaningful sentence representation\\r\\nwithout fine-tuning, since it was trained with NSP.\\n'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 4}, page_content='4175\\r\\n[CLS] my dog is cute [SEP] he likes play ##ing [SEP] Input\\r\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP] Emy Edog Eis Ecute E[SEP]\\r\\nToken\\r\\nEmbeddings\\r\\nEAEBEBEBEBEBEAEAEAEAEA\\r\\nSegment\\r\\nEmbeddings\\r\\nE0 E6E7E8E9E10 E1E2E3E4E5\\r\\nPosition\\r\\nEmbeddings\\r\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta\\x02tion embeddings and the position embeddings.\\r\\nThe NSP task is closely related to representation\\x02learning objectives used in Jernite et al. (2017) and\\r\\nLogeswaran and Lee (2018). However, in prior\\r\\nwork, only sentence embeddings are transferred to\\r\\ndown-stream tasks, where BERT transfers all pa\\x02rameters to initialize end-task model parameters.\\r\\nPre-training data The pre-training procedure\\r\\nlargely follows the existing literature on language\\r\\nmodel pre-training. For the pre-training corpus we\\r\\nuse the BooksCorpus (800M words) (Zhu et al.,\\r\\n2015) and English Wikipedia (2,500M words).\\r\\nFor Wikipedia we extract only the text passages\\r\\nand ignore lists, tables, and headers. It is criti\\x02cal to use a document-level corpus rather than a\\r\\nshuffled sentence-level corpus such as the Billion\\r\\nWord Benchmark (Chelba et al., 2013) in order to\\r\\nextract long contiguous sequences.\\r\\n3.2 Fine-tuning BERT\\r\\nFine-tuning is straightforward since the self\\x02attention mechanism in the Transformer al\\x02lows BERT to model many downstream tasks—\\r\\nwhether they involve single text or text pairs—by\\r\\nswapping out the appropriate inputs and outputs.\\r\\nFor applications involving text pairs, a common\\r\\npattern is to independently encode text pairs be\\x02fore applying bidirectional cross attention, such\\r\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\r\\ninstead uses the self-attention mechanism to unify\\r\\nthese two stages, as encoding a concatenated text\\r\\npair with self-attention effectively includes bidi\\x02rectional cross attention between two sentences.\\r\\nFor each task, we simply plug in the task\\x02specific inputs and outputs into BERT and fine\\x02tune all the parameters end-to-end. At the in\\x02put, sentence A and sentence B from pre-training\\r\\nare analogous to (1) sentence pairs in paraphras\\x02ing, (2) hypothesis-premise pairs in entailment, (3)\\r\\nquestion-passage pairs in question answering, and\\r\\n(4) a degenerate text-∅ pair in text classification\\r\\nor sequence tagging. At the output, the token rep\\x02resentations are fed into an output layer for token\\x02level tasks, such as sequence tagging or question\\r\\nanswering, and the [CLS] representation is fed\\r\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\r\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\r\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\r\\nfound in Appendix A.5.\\r\\n4 Experiments\\r\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\r\\n4.1 GLUE\\r\\nThe General Language Understanding Evaluation\\r\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\r\\ntasks. Detailed descriptions of GLUE datasets are\\r\\nincluded in Appendix B.1.\\r\\nTo fine-tune on GLUE, we represent the input\\r\\nsequence (for single sentence or sentence pairs)\\r\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\r\\nH corresponding to the first\\r\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\r\\nfine-tuning are classification layer weights W ∈\\r\\nR\\r\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\r\\ni.e., log(softmax(CWT)).\\r\\n7\\r\\nFor example, the BERT SQuAD model can be trained in\\r\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\r\\nF1 score of 91.0%.\\r\\n8\\r\\nSee (10) in https://gluebenchmark.com/faq.\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Splitter"
      ],
      "metadata": {
        "id": "JvrLsoivTulb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_doc=chunk_data(docs=pdf)\n",
        "len(pdf_doc)"
      ],
      "metadata": {
        "id": "5j9NMbSCbMyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06034e4-b2ca-43c2-e3f8-1a033f7f1d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Belgeyi parçalara ayırın\n",
        "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
        "#docs = text_splitter.split_documents(pdf)"
      ],
      "metadata": {
        "id": "53mpwb7KbMrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)"
      ],
      "metadata": {
        "id": "r0PsvQRsFVbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type='stuff'\n",
        ")\n",
        "output_summary = chain.invoke(pdf_doc[0:5])['output_text']"
      ],
      "metadata": {
        "id": "B6mNWgRAFY6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "lf0QW7u2FdU0",
        "outputId": "1aa7ee8c-bf25-4d4e-8822-f920b7e6e172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. Unlike previous models that use unidirectional context, BERT pre-trains deep bidirectional representations from unlabeled text, allowing it to consider both left and right context in all layers. This design enables BERT to be fine-tuned with minimal additional architecture for various natural language processing tasks, achieving state-of-the-art results on eleven benchmarks, including significant improvements in GLUE, MultiNLI, and SQuAD. BERT employs a masked language model pre-training objective, enhancing its effectiveness for both sentence-level and token-level tasks."
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make A Brief Summary of The Entire Document With Chain_Types of \"map_reduce\" and \"refine\""
      ],
      "metadata": {
        "id": "3zlVe2iISX0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**chain_type = map_reduce**"
      ],
      "metadata": {
        "id": "vQkcTpuTFlPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap\n",
        "\n",
        "llm = ChatOpenAI(temperature=0,\n",
        "                 model_name='gpt-4o-mini',\n",
        "                 max_tokens=1024)"
      ],
      "metadata": {
        "id": "t6wPW3OFbOcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
        "chunks = text_splitter.split_documents(pdf)"
      ],
      "metadata": {
        "id": "8NpJYGxRbOVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1uHb1M0Fu2G",
        "outputId": "103f8be1-5649-4190-f8b9-a5f2db41680c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbZuh-pFFx_U",
        "outputId": "e12dc392-df76-44c6-8abb-2fe1f4859c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/Rag_Chatbot/N19-1423.pdf', 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\r\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\rc 2019 Association for Computational Linguistics\\r\\n4171\\r\\nBERT: Pre-training of Deep Bidirectional Transformers for\\r\\nLanguage Understanding\\r\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\r\\nGoogle AI Language\\r\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\r\\nAbstract\\r\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\r\\nBidirectional Encoder Representations from\\r\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\r\\nunlabeled text by jointly conditioning on both\\r\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\r\\nto create state-of-the-art models for a wide\\r\\nrange of tasks, such as question answering and\\r\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\r\\nBERT is conceptually simple and empirically\\r\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\r\\ntasks, including pushing the GLUE score to\\r\\n80.5% (7.7% point absolute improvement),\\r\\nMultiNLI accuracy to 86.7% (4.6% absolute\\r\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\r\\n(5.1 point absolute improvement).\\r\\n1 Introduction\\r\\nLanguage model pre-training has been shown to\\r\\nbe effective for improving many natural language\\r\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\r\\n2018a; Radford et al., 2018; Howard and Ruder,\\r\\n2018). These include sentence-level tasks such as\\r\\nnatural language inference (Bowman et al., 2015;\\r\\nWilliams et al., 2018) and paraphrasing (Dolan\\r\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\r\\nholistically, as well as token-level tasks such as\\r\\nnamed entity recognition and question answering,\\r\\nwhere models are required to produce fine-grained\\r\\noutput at the token level (Tjong Kim Sang and\\r\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\r\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\r\\nfeature-based approach, such as ELMo (Peters\\r\\net al., 2018a), uses task-specific architectures that\\r\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\r\\nthe Generative Pre-trained Transformer (OpenAI\\r\\nGPT) (Radford et al., 2018), introduces minimal\\r\\ntask-specific parameters, and is trained on the\\r\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\r\\nsame objective function during pre-training, where\\r\\nthey use unidirectional language models to learn\\r\\ngeneral language representations.\\r\\nWe argue that current techniques restrict the\\r\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\r\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\r\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\r\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\r\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\r\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\r\\nIn this paper, we improve the fine-tuning based\\r\\napproaches by proposing BERT: Bidirectional\\r\\nEncoder Representations from Transformers.\\r\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\r\\nmasked language model randomly masks some of\\r\\nthe tokens from the input, and the objective is to\\r\\npredict the original vocabulary id of the masked')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"map_reduce\")\n",
        "\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "la6cz1pTFx7Y",
        "outputId": "e9bdd50c-bb79-4241-b6f7-95b2face355f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.17 s, sys: 243 ms, total: 2.41 s\n",
            "Wall time: 5min 22s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper presents BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model developed by Google AI Language. Unlike previous unidirectional models, BERT utilizes deep bidirectional representations by conditioning on both left and right contexts, allowing for effective pre-training on unlabeled text through tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). This architecture enables BERT to be fine-tuned with minimal modifications for various natural language processing (NLP) tasks, achieving state-of-the-art results on eleven benchmarks, including GLUE and SQuAD.\n\nBERT's input representation accommodates both single and paired sentences, employing WordPiece embeddings and special tokens for classification and separation. The model's performance is significantly enhanced by its extensive pre-training on large corpora, and it demonstrates superior accuracy across multiple tasks, particularly in scenarios with limited training data. The study also explores the impact of model size and pre-training tasks on performance, revealing that larger models and effective pre-training strategies lead to substantial improvements.\n\nOverall, BERT's innovative architecture and training methodology mark a significant advancement in NLP, with publicly available code and models facilitating further research and application in the field."
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**chain_type = refine**"
      ],
      "metadata": {
        "id": "sWnOyKbqF5Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.invoke(chunks)[\"output_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXC8XfxlF2Z-",
        "outputId": "95331618-3dbb-4772-fe2a-f33a0154ca65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.43 s, sys: 164 ms, total: 1.6 s\n",
            "Wall time: 5min 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnHQUsNiF2Md",
        "outputId": "fd64cae3-0aa7-40d0-f631-bd16b4c1abca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RefineDocumentsChain(initial_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ef9421d3ac0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ef9423b3fa0>, root_client=<openai.OpenAI object at 0x7ef9421de6e0>, root_async_client=<openai.AsyncOpenAI object at 0x7ef9421d3b50>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), refine_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['existing_answer', 'text'], template=\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ef9421d3ac0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ef9423b3fa0>, root_client=<openai.OpenAI object at 0x7ef9421de6e0>, root_async_client=<openai.AsyncOpenAI object at 0x7ef9421d3b50>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text', initial_response_name='existing_answer')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "fuw1tE8-GSQC",
        "outputId": "621e9888-23a0-414c-e199-2d35e1fc57df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The original summary is comprehensive and effectively captures the key aspects of the BERT model, its architecture, training methodology, and performance across various natural language processing tasks. The additional context provided elaborates on specific ablation studies regarding the number of training steps and different masking procedures used during pre-training. These studies indicate that BERT benefits from extensive pre-training, with significant accuracy improvements observed when increasing the number of training steps. Furthermore, the analysis of masking strategies reveals that while fine-tuning is robust to various masking approaches, the feature-based method is more sensitive to the choice of masking, highlighting the importance of the mixed strategy employed by BERT.\n\nHowever, these details do not significantly enhance the existing summary of BERT. Therefore, the original summary remains relevant and complete as it stands.\n\n**Final Summary:**\nThe paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. Unlike previous models that use unidirectional context, BERT pre-trains deep bidirectional representations from unlabeled text, allowing it to consider both left and right context in all layers. This design enables BERT to be fine-tuned with minimal additional architecture for various natural language processing tasks, achieving state-of-the-art results on eleven benchmarks, including significant improvements in GLUE, MultiNLI, and SQuAD tasks.\n\nBERT employs a masked language model (MLM) pre-training objective, which enhances its effectiveness for both sentence-level and token-level tasks. The model randomly masks a percentage of input tokens and predicts them, allowing for a deep bidirectional representation. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task that trains the model to understand the relationship between sentence pairs, which is crucial for tasks like Question Answering (QA) and Natural Language Inference (NLI). The NSP task is designed to transfer all parameters to initialize end-task model parameters, unlike prior work that only transferred sentence embeddings.\n\nTo handle various downstream tasks, BERT's input representation can unambiguously represent both single sentences and pairs of sentences. It uses WordPiece embeddings with a 30,000 token vocabulary, where the first token of every sequence is a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are differentiated using a special token ([SEP]) and learned embeddings indicating sentence membership. The input embeddings are the sum of token embeddings, segment embeddings, and position embeddings.\n\nBERT's architecture is a multi-layer bidirectional Transformer encoder, with two primary model sizes: BERTBASE and BERTLARGE. The model reduces the need for heavily-engineered task-specific architectures, marking it as the first fine-tuning based representation model to achieve state-of-the-art performance across a wide range of NLP tasks. Fine-tuning BERT is straightforward and can be accomplished in a relatively short time, with results replicable in about an hour on a single Cloud TPU or a few hours on a GPU.\n\nIn terms of performance, BERTBASE and BERTLARGE significantly outperform previous state-of-the-art models across various tasks, achieving average accuracy improvements of 4.5% and 7.0%, respectively, on the GLUE benchmark. For instance, BERTLARGE achieved an accuracy of 86.7% on the MNLI task, surpassing OpenAI GPT's 82.1%. In the SQuAD v1.1 question answering task, BERT demonstrated superior performance, with BERTLARGE (Single) achieving an F1 score of 90.9%, and an ensemble version reaching 91.8%, outperforming top leaderboard systems by notable margins. In SQuAD v2.0, BERT also showed a +5.1 F1 improvement over the previous best system, demonstrating its robustness in handling more complex question-answering scenarios.\n\nAblation studies highlighted the importance of the NSP task, showing that removing it significantly degrades performance on tasks like QNLI, MNLI, and SQuAD. Additionally, the impact of model size was explored, revealing that larger models consistently lead to accuracy improvements across various tasks, even those with limited training data. BERTBASE contains 110M parameters, while BERTLARGE contains 340M parameters, showcasing the effectiveness of scaling model size for enhanced performance.\n\nThe paper also discusses the potential of both fine-tuning and feature-based approaches with BERT. While the fine-tuning approach, where a simple classification layer is added to the pre-trained model, has shown significant success, the feature-based approach allows for the extraction of fixed features from the pre-trained model, which can be beneficial for certain tasks and computationally efficient. Experiments on the CoNLL-2003 Named Entity Recognition (NER) task demonstrated that BERT can perform competitively using both approaches, with the feature-based method achieving results close to those of fine-tuning.\n\nOverall, BERT's architecture and training methodology represent a significant advancement in the field of natural language processing, enabling effective transfer learning and improved"
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate A Detailed Summary of The Entire Document With At Least 1000 Tokens. Also, Add A Title To The Summary And Present Key Points Using Bullet Points With Chain_Type of \"map_reduce\"."
      ],
      "metadata": {
        "id": "9zZxse-ZUV3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**map_reduce with custom prompt**"
      ],
      "metadata": {
        "id": "0cVLJF2SGYyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce'\n",
        ")\n",
        "chain"
      ],
      "metadata": {
        "id": "lpuiEedJbQME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313534aa-857d-4c47-d511-26d965ea03f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ef9421d3ac0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ef9423b3fa0>, root_client=<openai.OpenAI object at 0x7ef9421de6e0>, root_async_client=<openai.AsyncOpenAI object at 0x7ef9421d3b50>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ef9421d3ac0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ef9423b3fa0>, root_client=<openai.OpenAI object at 0x7ef9421de6e0>, root_async_client=<openai.AsyncOpenAI object at 0x7ef9421d3b50>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)), document_variable_name='text')), document_variable_name='text')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for every chunk\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Map prompt (her parça için kullanılacak prompt)\n",
        "chunks_prompt = \"\"\"\n",
        "Please summarize the following text:\n",
        "{text}\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(input_variables=['text'], template=chunks_prompt)"
      ],
      "metadata": {
        "id": "52w9w_SPGjYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine prompt (bütün parçalar birleştirildiğinde kullanılacak prompt)\n",
        "final_combine_prompt = \"\"\"\n",
        "Summarize the text below with at least 1000 tokens. Provide a title and key points using bullet points:\n",
        "{text}\n",
        "\"\"\"\n",
        "final_combine_prompt_template = PromptTemplate(input_variables=['text'], template=final_combine_prompt)"
      ],
      "metadata": {
        "id": "OdFF9bOAbQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Map-Reduce zinciri oluşturma\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce',\n",
        "    map_prompt=map_prompt_template,\n",
        "    combine_prompt=final_combine_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "ASG41GGVGoO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
        "output_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "F0YBFbAGGskb",
        "outputId": "6de3a35e-fce0-4734-85d3-485ebe14658c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Title: Understanding BERT: A Comprehensive Overview of Bidirectional Encoder Representations from Transformers\\n\\n## Key Points:\\n\\n- **Introduction to BERT**: \\n  - BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking language representation model developed by Google AI Language.\\n  - It is designed to pre-train deep bidirectional representations from unlabeled text, taking into account both left and right context in all layers, which enhances its understanding of language.\\n\\n- **Advancements Over Previous Models**:\\n  - Traditional language models often utilized unidirectional approaches, which limited their contextual understanding and effectiveness for various tasks.\\n  - BERT introduces a \"masked language model\" (MLM) pre-training objective, which randomly masks tokens in the input text and trains the model to predict these masked tokens, thereby improving its contextual comprehension.\\n\\n- **Key Contributions of BERT**:\\n  - **Bidirectional Pre-training**: The MLM approach allows BERT to develop deep bidirectional representations, a significant improvement over previous unidirectional models.\\n  - **Reduction of Task-Specific Architectures**: BERT reduces the necessity for complex, task-specific architectures, achieving state-of-the-art performance across numerous NLP tasks through a fine-tuning process.\\n  - **Performance Improvement**: BERT has set new benchmarks for eleven NLP tasks, with its code and pre-trained models made publicly available for further research and application.\\n\\n- **Evolution of Language Representation Models**:\\n  - The text reviews the evolution of language representation models, highlighting the shift from traditional word embeddings to more advanced contextual models like ELMo and GPT, which paved the way for BERT\\'s development.\\n\\n- **BERT\\'s Architecture and Training Process**:\\n  - BERT\\'s training process consists of two main phases: pre-training on unlabeled data and fine-tuning on labeled data for specific downstream tasks.\\n  - The architecture is based on a multi-layer bidirectional Transformer encoder, which allows it to process input in both directions.\\n  - BERT is available in two sizes: BERTBASE and BERTLARGE, each with different configurations and total parameters.\\n\\n- **Input Representation**:\\n  - BERT can process both single sentences and pairs of sentences in a unified token sequence, utilizing WordPiece embeddings with a vocabulary of 30,000 tokens.\\n  - Each input sequence begins with a special classification token ([CLS]), and sentence pairs are separated using a special token ([SEP]) along with learned embeddings.\\n\\n- **Pre-training Tasks**:\\n  - **Masked Language Model (MLM)**: This task involves randomly masking a percentage of input tokens and training the model to predict them, which facilitates deep bidirectional representation.\\n  - **Next Sentence Prediction (NSP)**: This task trains the model to understand the relationship between sentences by predicting whether one sentence follows another.\\n\\n- **Fine-tuning Process**:\\n  - Fine-tuning BERT for various downstream tasks is efficient due to its self-attention mechanism, which allows it to handle both single texts and text pairs seamlessly.\\n  - The fine-tuning process involves adapting the model to specific tasks by modifying the input and output layers.\\n\\n- **Performance on Benchmarks**:\\n  - BERT models have significantly outperformed previous state-of-the-art systems on benchmarks such as the GLUE benchmark and the Stanford Question Answering Dataset (SQuAD v1.1).\\n  - BERTLARGE achieved an average accuracy improvement of 7.0% over prior models on GLUE tasks.\\n  - In SQuAD v1.1, BERT fine-tuned to predict answer spans from passages based on questions demonstrated superior results compared to leading systems.\\n\\n- **SQuAD and SWAG Results**:\\n  - BERTLARGE achieved an F1 score of 87.4 on the SQuAD 1.1 development set and 93.2 on the test set.\\n  - In SQuAD 2.0, BERTLARGE achieved an F1 score of 80.0, indicating significant improvement over previous systems.\\n  - BERTLARGE also excelled in the SWAG dataset, achieving an accuracy of 86.3.\\n\\n- **Ablation Studies**:\\n  - Ongoing research includes ablation studies to understand the contributions of different components of the BERT model.\\n  - Removing the NSP task significantly degrades performance, particularly on tasks like QNLI, MNLI, and SQuAD.\\n  - Increasing the model size consistently yields better performance across various datasets, even for tasks with limited training data.\\n\\n- **Feature-Based vs. Fine-Tuning Approaches**:\\n  - The text compares the effectiveness of fine-tuning all parameters versus using fixed features extracted from the pre-trained model for tasks like Named Entity Recognition (NER).\\n  - The feature-based method offers computational advantages and flexibility, demonstrating BERT\\'s effectiveness in both fine-tuning and feature extraction.\\n\\n- **Conclusion**:\\n  - The findings emphasize the importance of rich, unsupervised pre-training in enhancing language understanding systems'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gpLko_SWGuxz",
        "outputId": "1c765733-2f64-49fd-b21d-2c027957a544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Title: Understanding BERT: A Comprehensive Overview of Bidirectional Encoder Representations from Transformers\n\n## Key Points:\n\n- **Introduction to BERT**: \n  - BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking language representation model developed by Google AI Language.\n  - It is designed to pre-train deep bidirectional representations from unlabeled text, taking into account both left and right context in all layers, which enhances its understanding of language.\n\n- **Advancements Over Previous Models**:\n  - Traditional language models often utilized unidirectional approaches, which limited their contextual understanding and effectiveness for various tasks.\n  - BERT introduces a \"masked language model\" (MLM) pre-training objective, which randomly masks tokens in the input text and trains the model to predict these masked tokens, thereby improving its contextual comprehension.\n\n- **Key Contributions of BERT**:\n  - **Bidirectional Pre-training**: The MLM approach allows BERT to develop deep bidirectional representations, a significant improvement over previous unidirectional models.\n  - **Reduction of Task-Specific Architectures**: BERT reduces the necessity for complex, task-specific architectures, achieving state-of-the-art performance across numerous NLP tasks through a fine-tuning process.\n  - **Performance Improvement**: BERT has set new benchmarks for eleven NLP tasks, with its code and pre-trained models made publicly available for further research and application.\n\n- **Evolution of Language Representation Models**:\n  - The text reviews the evolution of language representation models, highlighting the shift from traditional word embeddings to more advanced contextual models like ELMo and GPT, which paved the way for BERT's development.\n\n- **BERT's Architecture and Training Process**:\n  - BERT's training process consists of two main phases: pre-training on unlabeled data and fine-tuning on labeled data for specific downstream tasks.\n  - The architecture is based on a multi-layer bidirectional Transformer encoder, which allows it to process input in both directions.\n  - BERT is available in two sizes: BERTBASE and BERTLARGE, each with different configurations and total parameters.\n\n- **Input Representation**:\n  - BERT can process both single sentences and pairs of sentences in a unified token sequence, utilizing WordPiece embeddings with a vocabulary of 30,000 tokens.\n  - Each input sequence begins with a special classification token ([CLS]), and sentence pairs are separated using a special token ([SEP]) along with learned embeddings.\n\n- **Pre-training Tasks**:\n  - **Masked Language Model (MLM)**: This task involves randomly masking a percentage of input tokens and training the model to predict them, which facilitates deep bidirectional representation.\n  - **Next Sentence Prediction (NSP)**: This task trains the model to understand the relationship between sentences by predicting whether one sentence follows another.\n\n- **Fine-tuning Process**:\n  - Fine-tuning BERT for various downstream tasks is efficient due to its self-attention mechanism, which allows it to handle both single texts and text pairs seamlessly.\n  - The fine-tuning process involves adapting the model to specific tasks by modifying the input and output layers.\n\n- **Performance on Benchmarks**:\n  - BERT models have significantly outperformed previous state-of-the-art systems on benchmarks such as the GLUE benchmark and the Stanford Question Answering Dataset (SQuAD v1.1).\n  - BERTLARGE achieved an average accuracy improvement of 7.0% over prior models on GLUE tasks.\n  - In SQuAD v1.1, BERT fine-tuned to predict answer spans from passages based on questions demonstrated superior results compared to leading systems.\n\n- **SQuAD and SWAG Results**:\n  - BERTLARGE achieved an F1 score of 87.4 on the SQuAD 1.1 development set and 93.2 on the test set.\n  - In SQuAD 2.0, BERTLARGE achieved an F1 score of 80.0, indicating significant improvement over previous systems.\n  - BERTLARGE also excelled in the SWAG dataset, achieving an accuracy of 86.3.\n\n- **Ablation Studies**:\n  - Ongoing research includes ablation studies to understand the contributions of different components of the BERT model.\n  - Removing the NSP task significantly degrades performance, particularly on tasks like QNLI, MNLI, and SQuAD.\n  - Increasing the model size consistently yields better performance across various datasets, even for tasks with limited training data.\n\n- **Feature-Based vs. Fine-Tuning Approaches**:\n  - The text compares the effectiveness of fine-tuning all parameters versus using fixed features extracted from the pre-trained model for tasks like Named Entity Recognition (NER).\n  - The feature-based method offers computational advantages and flexibility, demonstrating BERT's effectiveness in both fine-tuning and feature extraction.\n\n- **Conclusion**:\n  - The findings emphasize the importance of rich, unsupervised pre-training in enhancing language understanding systems"
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXrrUJutGxMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "FLJmnz9TVCRL"
      }
    }
  ]
}